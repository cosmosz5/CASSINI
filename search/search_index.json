{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to CASSINI - A project to develop image reconstruction tools based con Compressed Sensing, Pincipal Components an Neural Networks Link to the GitHub repository with the code, click HERE The project Interferometry delivers the highest angular resolution in Astronomy. Since the 1960s, it is being used extensively in radio astronomy and, since about a couple of decades, it has become an important player in infrared astronomy. However, infrared interferometry is restricted to sparse arrays of only a few telescopes. While imaging is arguably the most intuitive way to analyze interferometric data, recovering images from sparsely sampled visibilities is challenging. Which is the impact of the sparse u-v coverage on imaging? The sparse u-v coverage obtained with infrared arrays requieres complementary \u201ca-priori\u201d information to recover an image. From signal theory, interferometric imaging is an \u201cill-possed\u201d problem, even monochromatic images as small as 128x128 pixels requiere at least 16384 visibilities to obtain an independent solution. This number of data points is virtually imposible to obtain, given the current number of telescopes that forms infrared arrays. For example, the Very Large Telescope Interferometer (VLTI) can only provide up to 6 visibility points (per wavelength) per snapshot. Therefore, typically, there are more unknowns (or pixel values) than equations (u-v data) to solve the imaging problem. This constraint on the number of visibility points could be relaxed, if we consider that the pixels in astronomical images are not independent because we are observing highly structured morphologies. We can take advantage of the compressibility of the brightness distribution in the Fourier space and, hence, to use considerably less number of visibility points for retrieving an image. However, we would still require assumptions on the target\u2019s morphology to achieve a reliable imaging solution. How does the lack of full-phase information influence imaging? At infrared wavelengths, the atmosphere plays an important role for interferometric observations. The millisecond coherence time and the photon starved regime of the observations make virtually imposible to retrieve reliable Fourier phases as interferometric observables. In contrast with radio observations, the argument of the bispectrum (often called closure phases) of the visibilities is used for retrieving information on the centro-symmetric asymmetries of the source. One of the most important limitation of this observable is that it is shift-invariable to the position of the source in the pixel grid. This means that the image of a source could be formed at any position in the pixel grid and the resultant closure phases will be the same. This implies that, while relative astrometry is retrieved, the absolute astrometry of the imaged object lost. The limited phase information does not allow us to use direct Fourier inversion techniques (such as CLEAN) to recover images. In contrast, regularized minimization algorithms over the pixel values are required. This project aims at investigating new methodologies and algorithms for interferometric image reconstruction based on the theory of Compressed Sensing and the novel implementation of compressibility of a signal trhough Neural Networks to retrieve more reliable interferometric images from sparse arrays. Project layout CASSINI # The main directory of the project SAMPip/ # This directory contains the software to perform data # reduction of Aperture Masking Data. It also contains JWST SAM data # on WR137 to run the example illustrated in this documentation NN_imaging/ # This directory contains the software to perform # image reconstruction using a Convotutional Neural Network called AUTOMAP RCAR_analysis/ # This directory contains the software # and tools used for analyzing GRAVITY-VLTI data of the AGB star R Car LASSO/ # This directory contains the software tools to perform # interferometric imaging based on Compressed Sensing CS_PCA/ # This directory contains the software to obtain the # principal components of images and their corresponding Fourier # transforms. Installation There is no need for specific installation of the code. Each sub-package is self-contained. To use each one of them, it is as simple as clone the main repository folder with the following command: >> git clone https://github.com/cosmosz5/CASSINI.git However, each sub-package made use of the different Python modules available in the community. It is necessary that the user install them separately. Below, there is a description of the different requirements for each sub-module: Requirements (all the CASSINI codes work with Python 3.x) Python modules for CASSINI/SAMPip The main module to run the code is test.py . This script defines the input parameters and invoke the necesary code to run SAMPip. The user can modify this script to adapt it to his/her necessities. The user can run it on the Terminal by typing: >> python test.py Required packages: numpy astropy cvxpy sklearn skimage pylops test.py finishes in debug mode. To get out of it, the user should type \"q\" in the Terminal. Python modules for CASSINI/NN_imaging The main module to run the code is AUTOMAP_keras_4.py . This script run the Convolutional Neural Network for imaging. The user can modify this script to adapt it to his/her necessities. The user can run it on the Terminal (to start the trainning process) by typing: >> python python AUTOMAP_keras_4.py --mode train --epochs 300 --batch-size 128 Required packages: plaidml keras astropy numpy argparse math oitools Python modules for CASSINI/RCAR_analysis This module include all the tools developed to analyze parametric models and reconstructed images for the analysis of the morphology of AGB stars. This module includes a Python jupyter notebook called RCar_analysis.ipynb to run all the examples included in the repository. Required packages installed: astropy emcee lmfit matplotlib numpy Open CV scipy sklearn Python modules for CASSINI/LASSO The main module to run the code is CS_JWST_v1.py . This script defines the input parameters and invoke the necesary code to run the compressed sensins imaging. To run the code on CASSINI/LASSO it is necessary to have the DATABASE of ring models for the dictionary to work. A part of this database is included in the repository. However, due to space resctictions on GitHub, the complete version of the DATABASE can be found in the this link . The user can modify this script to adapt it to his/her necessities. The user can run it on the Terminal by typing: >> python CS_JWST_v1.py Required packages installed: numpy astropy matplotlib pylab sklearn CS_JWST_v1.py finishes in a debug mode. To get out of it, the user should type \"q\" in the terminal Python modules for CASSINI/CS_PCA This module contains tools to extract the Principal Components of a reconstructed image and of its Fourier transform. The main module to run the code is PCA_im.py . The user can modify this script to adapt it to his/her necessities (see also CASSINI-GRAVITY RCAR analysis for an additional example on how to use CASSINI-PCA). The user can run it on the Terminal by typing: >> python PCA_im.py Required packages: cvxpy sklearn PyLops numpy DISCLAIMER This project has been developed with funding from the UNAM PAPIIT project IA 101220 and from the Mexico's National Council of Humanities, Science and Technology (CONACyT) \u201cCiencia de Frontera\u201d project 263975. All the scripts that compose CASSINI are open source under GNU License. For enquiries and/or contributions please contact joelsb@astro.unam.mx","title":"Home"},{"location":"#welcome-to-cassini-a-project-to-develop-image-reconstruction-tools-based-con-compressed-sensing-pincipal-components-an-neural-networks","text":"","title":"Welcome to CASSINI - A project to develop image reconstruction tools based con Compressed Sensing, Pincipal Components an Neural Networks"},{"location":"#link-to-the-github-repository-with-the-code-click-here","text":"","title":"Link to the GitHub repository with the code, click HERE"},{"location":"#the-project","text":"Interferometry delivers the highest angular resolution in Astronomy. Since the 1960s, it is being used extensively in radio astronomy and, since about a couple of decades, it has become an important player in infrared astronomy. However, infrared interferometry is restricted to sparse arrays of only a few telescopes. While imaging is arguably the most intuitive way to analyze interferometric data, recovering images from sparsely sampled visibilities is challenging. Which is the impact of the sparse u-v coverage on imaging? The sparse u-v coverage obtained with infrared arrays requieres complementary \u201ca-priori\u201d information to recover an image. From signal theory, interferometric imaging is an \u201cill-possed\u201d problem, even monochromatic images as small as 128x128 pixels requiere at least 16384 visibilities to obtain an independent solution. This number of data points is virtually imposible to obtain, given the current number of telescopes that forms infrared arrays. For example, the Very Large Telescope Interferometer (VLTI) can only provide up to 6 visibility points (per wavelength) per snapshot. Therefore, typically, there are more unknowns (or pixel values) than equations (u-v data) to solve the imaging problem. This constraint on the number of visibility points could be relaxed, if we consider that the pixels in astronomical images are not independent because we are observing highly structured morphologies. We can take advantage of the compressibility of the brightness distribution in the Fourier space and, hence, to use considerably less number of visibility points for retrieving an image. However, we would still require assumptions on the target\u2019s morphology to achieve a reliable imaging solution. How does the lack of full-phase information influence imaging? At infrared wavelengths, the atmosphere plays an important role for interferometric observations. The millisecond coherence time and the photon starved regime of the observations make virtually imposible to retrieve reliable Fourier phases as interferometric observables. In contrast with radio observations, the argument of the bispectrum (often called closure phases) of the visibilities is used for retrieving information on the centro-symmetric asymmetries of the source. One of the most important limitation of this observable is that it is shift-invariable to the position of the source in the pixel grid. This means that the image of a source could be formed at any position in the pixel grid and the resultant closure phases will be the same. This implies that, while relative astrometry is retrieved, the absolute astrometry of the imaged object lost. The limited phase information does not allow us to use direct Fourier inversion techniques (such as CLEAN) to recover images. In contrast, regularized minimization algorithms over the pixel values are required. This project aims at investigating new methodologies and algorithms for interferometric image reconstruction based on the theory of Compressed Sensing and the novel implementation of compressibility of a signal trhough Neural Networks to retrieve more reliable interferometric images from sparse arrays.","title":"The project"},{"location":"#project-layout","text":"CASSINI # The main directory of the project SAMPip/ # This directory contains the software to perform data # reduction of Aperture Masking Data. It also contains JWST SAM data # on WR137 to run the example illustrated in this documentation NN_imaging/ # This directory contains the software to perform # image reconstruction using a Convotutional Neural Network called AUTOMAP RCAR_analysis/ # This directory contains the software # and tools used for analyzing GRAVITY-VLTI data of the AGB star R Car LASSO/ # This directory contains the software tools to perform # interferometric imaging based on Compressed Sensing CS_PCA/ # This directory contains the software to obtain the # principal components of images and their corresponding Fourier # transforms.","title":"Project layout"},{"location":"#installation","text":"There is no need for specific installation of the code. Each sub-package is self-contained. To use each one of them, it is as simple as clone the main repository folder with the following command: >> git clone https://github.com/cosmosz5/CASSINI.git However, each sub-package made use of the different Python modules available in the community. It is necessary that the user install them separately. Below, there is a description of the different requirements for each sub-module:","title":"Installation"},{"location":"#requirements","text":"(all the CASSINI codes work with Python 3.x)","title":"Requirements"},{"location":"#python-modules-for-cassinisampip","text":"The main module to run the code is test.py . This script defines the input parameters and invoke the necesary code to run SAMPip. The user can modify this script to adapt it to his/her necessities. The user can run it on the Terminal by typing: >> python test.py Required packages: numpy astropy cvxpy sklearn skimage pylops test.py finishes in debug mode. To get out of it, the user should type \"q\" in the Terminal.","title":"Python modules for CASSINI/SAMPip"},{"location":"#python-modules-for-cassininn_imaging","text":"The main module to run the code is AUTOMAP_keras_4.py . This script run the Convolutional Neural Network for imaging. The user can modify this script to adapt it to his/her necessities. The user can run it on the Terminal (to start the trainning process) by typing: >> python python AUTOMAP_keras_4.py --mode train --epochs 300 --batch-size 128 Required packages: plaidml keras astropy numpy argparse math oitools","title":"Python modules for CASSINI/NN_imaging"},{"location":"#python-modules-for-cassinircar_analysis","text":"This module include all the tools developed to analyze parametric models and reconstructed images for the analysis of the morphology of AGB stars. This module includes a Python jupyter notebook called RCar_analysis.ipynb to run all the examples included in the repository. Required packages installed: astropy emcee lmfit matplotlib numpy Open CV scipy sklearn","title":"Python modules for CASSINI/RCAR_analysis"},{"location":"#python-modules-for-cassinilasso","text":"The main module to run the code is CS_JWST_v1.py . This script defines the input parameters and invoke the necesary code to run the compressed sensins imaging. To run the code on CASSINI/LASSO it is necessary to have the DATABASE of ring models for the dictionary to work. A part of this database is included in the repository. However, due to space resctictions on GitHub, the complete version of the DATABASE can be found in the this link . The user can modify this script to adapt it to his/her necessities. The user can run it on the Terminal by typing: >> python CS_JWST_v1.py Required packages installed: numpy astropy matplotlib pylab sklearn CS_JWST_v1.py finishes in a debug mode. To get out of it, the user should type \"q\" in the terminal","title":"Python modules for CASSINI/LASSO"},{"location":"#python-modules-for-cassinics_pca","text":"This module contains tools to extract the Principal Components of a reconstructed image and of its Fourier transform. The main module to run the code is PCA_im.py . The user can modify this script to adapt it to his/her necessities (see also CASSINI-GRAVITY RCAR analysis for an additional example on how to use CASSINI-PCA). The user can run it on the Terminal by typing: >> python PCA_im.py Required packages: cvxpy sklearn PyLops numpy","title":"Python modules for CASSINI/CS_PCA"},{"location":"#disclaimer","text":"This project has been developed with funding from the UNAM PAPIIT project IA 101220 and from the Mexico's National Council of Humanities, Science and Technology (CONACyT) \u201cCiencia de Frontera\u201d project 263975. All the scripts that compose CASSINI are open source under GNU License. For enquiries and/or contributions please contact joelsb@astro.unam.mx","title":"DISCLAIMER"},{"location":"NN/","text":"This repository includes software to reduce data obtained with Fizeau interferometry in the form of Sparse Aperture Masking. Link to the GitHub repository with the code, click HERE 1. Introduction to the current state of infrared interferometric imaging During the last decade, there has been a large effort to provide reliable software for image reconstruction to the community. However, the existing software packages face some problems: \u2022 Most of the algorithms only work with monochromatic reconstructions. They cannot simultaneously recover images over different wavelengths. Therefore, they cannot make full use of differential phases and astrometry. \u2022 Most of the algorithms rely on regularization functions in image space. These functions are neccessary to ensure convergence and impose conditions such as smooth transitions between adjacent pixels, positivity, or sharp edges. Unfortunately, an unsuitable choice in the selection of regularization functions can result in in different outcomes for the same target even in the presence of a very good u-v coverage. Finally, ideally regularization functions need to vary across the pixel grid in case of complex target morphologies. This poses additional difficulties for the choice of regularizers and for the convergence of the algorithms, which are intimately connected. \u2022 Most existing software packages use a Bayesian approach to fit the pixels\u2019 brightness distribution. However, the fit is not always well constrained because of the traditional minimization algorithms used (gradient descent and Monte-Carlo), in particular if non-convex regularizers (such as the L0-norm) are used for the reconstruction. This can prevent the convergence of the algorithms. \u2022 Each attempt at image reconstruction resembles an \u201cartisanal\u201d process, relying on the experience of each individual astronomer. Each reconstruction is tailored for a specific target and it is generally not possible to apply similar setups to a group of targets. This limits the potential of imaging for studies of large numbers of objects severely. A transition from artisanal to standard and reproducible imaging reconstruction is needed, securing the reliability and reproducibility of the results. This is particularly important for the upcoming instrumentation such as GRAVITY+ at the VLTI and NIRISS-SAM at the JWST. 2. Neural Networks for interferometric imaging 2.1. Introduction Artificial neural networks (NN) are highly distributed systems that are inspired by the way neurons are inter- connected in biological nerve systems. The basic unit in a NN is called a neuron. It is interconnected with others to learn certain patterns or predict certain behavior. For this, it is neccessary to train the NN over several iterations using a large number of models or measurements. This type of algorithm has demonstrated to learn very complex patterns that cannot be easily predicted with other, more traditional, methods. A type of NN that has demonstrated to be very efficient in learning morphological structures from images is a Convolutional Neural Network (CNN) . This type of network is designed to work with grid-structured inputs that have strong local spatial correlations like the pixels of structured astronomical images. The key of the success of CNNs is their ability to semantically understand different structures by connecting layers in a localized way. Some examples of the applicability of these networks to recover, or even to generate new \u201dnatural\u201d images are the so-called Generative Adversarial Networks or Generative Variational Auto Encoders. Fig. 1 shows an example of a generative adversarial network, where two networks, the Generator (G; which is typically a CNN) and the Discriminator (D), play a minmax game so that G can learn how to produce new images based on the morphological properties of the ones used for training. 2.2. Adaptive hyper-parameters, a new framework for Neural Network training The previous approaches rely on the process of fitting an object representation to noisy data by using regularized minimization methods (either by using the sparsity as regularizer, like in the CS approach; or using the output of D in the GAN reconstruction). However, a machine-learning step-forward is to estimate a direct mapping, F(m; \u03b8) , parametrized by \u03b8 , from the interferometric data, m , to the reconstructed image, t , without pre-defining strong hyperparameters. In the NN framework, to find \u03b8 this approach requires (i) to learn the set of weights of the neuron connections during the training process and (ii) to properly define the activation functions of each one of the neurons, when designing the network architecture. One of the key aspects of pattern recognition from NNs is the role of the activation function. The activation function decides, whether a neuron should be activated or not with the purpose of introducing non-linearity into the output of a neuron. Determining the different activation functions in the architecture of the network is essential for its correct functioning. There are, at least, three types of activation functions: saturated, unsaturated and adaptive. Saturated activation functions are used for decision boundaries (e.g., Sigmoid). However, this type of activation functions could lead to vanishing gradient problems during the training process. Unsaturated activation functions solve the vanishing gradient problem but, it they fall into a negative region, is highly improbable that the neuron could be activated again (e.g., Rectified Linear Unit -ReLU-). This could be solved by adding a constant term that acts like threshold for the negative part of the function (e.g., Leaky ReLU). Nevertheless, including this new pre-defined constant is like adding an hyperparameter which could strongly affect the learning process of the NN. Adaptive activation functions solve these problems by using trainable coefficients. The use of adaptive activations functions has demonstrated to have interesting implications in making the learning process faster and more effective. Figure 1. Schematics of a Generative Adversarial Network (GAN). This type of neural network confronts two nets in a minmax game. The first one is the generator (G) which is going to create new samples of a signal from a compressed dimensional space (i.e., the latent space). The second one is the discriminator (D), which tries to determine whether the input from G is fake in comparison with a sample or real images. 3. Example of NN imaging 3.1. Observations and Data The data added to illustrate our NN reconstruction approach correspond to Sparse Aperture Masking (SAM) observations were taken with the NACO-VLT infrared camera. Particularly, we recorded SAM data on the extended infrared source GC IRS 1W in the Galactic Center (GC). This target is a previously identified stellar bow-shock source produced by the relative motion of the central star and the momentum balance shock between the stellar wind and the interstellar medium. The observations were performed using the L27 camera (0.027\u201d/pixel) with the L\u2019 broad-band filter (\u03bb0 = 3.80 \u03bcm, \u2206\u03bb = 0.62 \u03bcm) combined with the BB 9holes NRM. We obtained 4 different data cubes from which the interferometric observable where extracted using CASSINI-SAMPip . Fig. 2 displays the calibrated squared visibilities, closure phases and u-v plane. The file MERGED_IRS1W.oifits contains all the SAM data used in this example and it is included in the Github repository. Figure 1. The lef t panel shows the calibrated squared visibilities of GC IRS 1w versus spatial frequencies obtained from our NACO-SAM observations. Similarly, the middle panel shows the calibrated closure phases versus spatial frequencies. The right panel displays the u-v plane of the observations. 3.2. Software architecture Our NN algorithm is based on the AUTOMAP architecture. The AUTOMAP network has been used to reconstruct images from medical data obtained through Magnetic Resonance Imaging (MRI). This network has demonstrated to be quite efficient in mapping between sensor and image domains with the use of appropriate training data. The network presented here, called CASSINI-AUTOMAP , follows this approach and it maps directly the interferometric data to the reconstructed image by using a convolutional neural network. Figure 3 displays the model summary with the different layers used and the number of parameters trained on each one. Figure 4 shows a schematic representation of our NN. In the diagram, we can observe the two main sections of the network. The first one corresponds to three fully connected layers (FC1, FC2 and FC3) which map the input observables with the number of pixels in the output image. FC3 has a dimension equivalent to n2 \u00d7 1, where n is the number of pixels per side in the output image, in our case n=64. FC1 uses a simple sigmoid activation function. FC2 and FC3 use an adaptive hyperbolic tangent (Atanh) activation function with two trainable parameters. The second section employs three convolutional layers (C1, C2 and C3). C1 and C2 convolve 64 filters of 5 \u00d7 5 kernel size with stride 1 while, C3 uses a 7 \u00d7 7 kernel size. C1 and C2 employ a parametric rectified linear unit (PReLU) activation function and dropout with a probability of 0.1 before being connected between them. C3 and the output layer, C , are used to deconvolve the image into a 64 \u00d7 64 array. Figure 3. The figure summarizes the different layers used in our CASSINI-AUTOMAP network. The first column displays the name of the different layers; the second column shows the output shape at a given layer and; the third column lists the number of trainable parameters at each layer. Figure 4. Schematic of the CASSINI-AUTOMAP network. The 1D and 2D sections are displayed with different colors. The output of the network correspond to a 64 \u00d7 64 image of a stellar bow shock. Our network was deployed in Python using the Keras module in an Apple Mac BookPro computer under the PlaidML environment. The training of the network was performed using an external AMD Radeon RX 570 GPU with 4 GB of memory. CASSINI-AUTOMAP is accessible in the Github repository of this documentation. It is a single script that can be run with the following command: >> python AUTOMAP_keras_4.py --mode train --epochs 300 --batch-size 128 IMPORTANT: before running the previous command, it is neccessary to have the following two files: xnew_OBTot.npz and bs64_flux.npz in the sub-directory NN_imaging of CASSINI . Due to limitations in the size of the files uploaded to Github, those files can be retrieved from this Dropbox folder . As it can be observed AUTOMAP runs in-line on the Terminal with the following available commands: 1. --mode (train, generate, validate) 2. --batch_size (integer value) 3. --nice (True, False) 4. --epoch (integer value) 5. --oifilename (filename of the oifits data when recover interferometric images from data) As part of the repository a .npz file is included with 2000 compressed radiative transfer models that can be used to train the network. These models are acompained with their corresponding interferometric observables. Both files are hardcoded in the NN code, they are asigned to the variables: load_data = np.load('bs64_flux.npz') ## The RT models load_oidata = np.load('xnew_OBTot.npz') ## The interferometric observables from the RT models This setup will allow us to fully train the network over 300 epochs using the Adam optimizer. From our tests, we notice that the number of epochs used is, at least, a third of the ones required without using adaptive activation functions. This results in a significant reduction of the processing time required during training. The cost function used for back-propagation of the gradient is the regression mean-squared error (MSE) loss. Figure 5 displays the MSE obtained from the different batches per epoch over the training process. The evolution of the MSE converges after 300 epochs. The biggest jump is observed around the first 20 epochs. This coincides with the epoch when the network learns the general shape of the bow shock. Figure 5. Evolution of the cost function (mean-square error) over the 300 epochs in which the CASSINI-AUTOMAP network was trained. Every 50 epochs the code saves the state of the weights in the network. In case the user needs to stop the net for debugging process. Every ten epochs the code also saves a .png file with a random series of batch images with the state of the trainning process; the code also saves a .fits file with those images for further inspection by the user. Fig. 6 displays an example of the images generated. Figure 6. The image displays different panels obtained during the training process of AUTOMAP. Different morphologies of the bowshocks are recovered (i.e., different sizes, position angles, inclinations, brigthness, etc). To illustrate better how the trainning process work, we plotted in Fig. 7 a sample of 5 different images obtained at different epochs, that clearly illustrate the evolution of the training process, at Epoch 0 there is only noise in the predicted images. However, as the network weights are evolving, central elongated structures appear and gradually the form of bow shocks emerge. Figure 7. The image displays 25 panels with reconstructed images obtained with CASSINI-AUTOMAP. Every column shows 5 images at different epochs. Each panel covers an equivalent area of 640 \u00d7 640 mas. The emission is normalized to the peak of each image. 3.3 Reconstruction and benckmarking Once the trainning process is finished (it is important that the user validates - this is done with the --mode validate keyword - the trainning of the net using independent models), AUTOMAP could be used to recover an image with real data. to do this, the user should include the following instruction in the Terminal: >> python AUTOMAP_keras_4.py --mode generate --oifilename MERGED_IRS1W.oifits where MERGED_IRS1W.oifits is the OIFITS file with the calibrated data that we are using for the reconstruction. For this example, we predict the morphology of the GC IRS 1W bow shock by randomly creating 100 samples from the interferometric data, assuming Gaussian distributions based on the mean and standard deviation of each observable. The mean image obtained with CASSINI-AUTOMAP was able to recover the interferometric observables with great accuracy. Indeed, the morphology of the bow-shock is clearly visible, even for pixel values below 5% of the peak value. We estimated a SNR \u223c 40 for pixel values of 5% of the emission peak. In order to benchmark our neural network imaging algorithm, images from the SAM data were reconstructed using the regularized minimization algorithm BSMEM (21; 22). This code uses entropy as regularizer to encode the prior information of the source\u2019s brightness distribution. This code uses the squared visibilities, closure phases and closure amplitudes to fit the model images with the data. Images were recovered using a pixel scale of 10 mas/px over a pixel grid of 64 \u00d7 64 px. Fig. 8 displays the best reconstructed image using BSMEM (the code converges to a \u03c72 = 1.1). Notice how the structure of the bow shock is recovered. The apex and the tails of the morphology are clearly observed. It can be seen that the peak of the brightness distribution is over the tails of the bow shock. The synthetic observables from the reconstructed images are displayed on top of the interferometric observables from the data. The best-fit image appears to recover quite well the trend observed in the interferometric data. The reconstructed image with the NN (Fig. 9) also exhibits an asymmetric bow shock with the peak of the emission over the tails of the shock front. The estimated size of the bow shock and the position angle of the apex (PA \u223c 25\u00ba) are similar between the BSMEM image and the CASSINI-AUTOMAP one. However, we could clearly identify that the emission below 10% of the peak value in the BSMEM image is very asymmetric, while, in the NN image the emission follows the same structure of the tails. This effect is caused because of the models employed for training the network. However, this does not compromise the fit of the observables. Adding more structures (like point-like sources) to the models used for training could help us to discover additional asymmetries in the source structure. The interesting idea behind this is the possibility of linking these additional sources with well-recognized physical structures, for example dust clumps or stellar companions. Figure 8. Left panel: Normalized BSMEM reconstructed image from our SAM data. The white contours show the 5, 10, 30, 50, 70 and 90 % of the source\u2019s peak. Middle and right panels: Squared visibilities and closure phases versus spatial frequencies. The NACO-SAM data is displayed with black dots and the observables extracted from the reconstructed image are displayed with red dots. Figure 9. Left panel: Normalized CASSINI-AUTOMAP reconstructed image from our SAM data. The white contours show the 5, 10, 30, 50, 70 and 90 % of the source\u2019s peak. Middle and right panels: Squared visibilities and closure phases versus spatial frequencies. The NACO-SAM data is displayed with black dots and the observables extracted from the reconstructed image are displayed with red dots.","title":"NN for imaging"},{"location":"NN/#this-repository-includes-software-to-reduce-data-obtained-with-fizeau-interferometry-in-the-form-of-sparse-aperture-masking","text":"","title":"This repository includes software to reduce data obtained with Fizeau interferometry in the form of Sparse Aperture Masking."},{"location":"NN/#link-to-the-github-repository-with-the-code-click-here","text":"","title":"Link to the GitHub repository with the code, click HERE"},{"location":"NN/#1-introduction-to-the-current-state-of-infrared-interferometric-imaging","text":"During the last decade, there has been a large effort to provide reliable software for image reconstruction to the community. However, the existing software packages face some problems: \u2022 Most of the algorithms only work with monochromatic reconstructions. They cannot simultaneously recover images over different wavelengths. Therefore, they cannot make full use of differential phases and astrometry. \u2022 Most of the algorithms rely on regularization functions in image space. These functions are neccessary to ensure convergence and impose conditions such as smooth transitions between adjacent pixels, positivity, or sharp edges. Unfortunately, an unsuitable choice in the selection of regularization functions can result in in different outcomes for the same target even in the presence of a very good u-v coverage. Finally, ideally regularization functions need to vary across the pixel grid in case of complex target morphologies. This poses additional difficulties for the choice of regularizers and for the convergence of the algorithms, which are intimately connected. \u2022 Most existing software packages use a Bayesian approach to fit the pixels\u2019 brightness distribution. However, the fit is not always well constrained because of the traditional minimization algorithms used (gradient descent and Monte-Carlo), in particular if non-convex regularizers (such as the L0-norm) are used for the reconstruction. This can prevent the convergence of the algorithms. \u2022 Each attempt at image reconstruction resembles an \u201cartisanal\u201d process, relying on the experience of each individual astronomer. Each reconstruction is tailored for a specific target and it is generally not possible to apply similar setups to a group of targets. This limits the potential of imaging for studies of large numbers of objects severely. A transition from artisanal to standard and reproducible imaging reconstruction is needed, securing the reliability and reproducibility of the results. This is particularly important for the upcoming instrumentation such as GRAVITY+ at the VLTI and NIRISS-SAM at the JWST.","title":"1. Introduction to the current state of infrared interferometric imaging"},{"location":"NN/#2-neural-networks-for-interferometric-imaging","text":"","title":"2. Neural Networks for interferometric imaging"},{"location":"NN/#21-introduction","text":"Artificial neural networks (NN) are highly distributed systems that are inspired by the way neurons are inter- connected in biological nerve systems. The basic unit in a NN is called a neuron. It is interconnected with others to learn certain patterns or predict certain behavior. For this, it is neccessary to train the NN over several iterations using a large number of models or measurements. This type of algorithm has demonstrated to learn very complex patterns that cannot be easily predicted with other, more traditional, methods. A type of NN that has demonstrated to be very efficient in learning morphological structures from images is a Convolutional Neural Network (CNN) . This type of network is designed to work with grid-structured inputs that have strong local spatial correlations like the pixels of structured astronomical images. The key of the success of CNNs is their ability to semantically understand different structures by connecting layers in a localized way. Some examples of the applicability of these networks to recover, or even to generate new \u201dnatural\u201d images are the so-called Generative Adversarial Networks or Generative Variational Auto Encoders. Fig. 1 shows an example of a generative adversarial network, where two networks, the Generator (G; which is typically a CNN) and the Discriminator (D), play a minmax game so that G can learn how to produce new images based on the morphological properties of the ones used for training.","title":"2.1. Introduction"},{"location":"NN/#22-adaptive-hyper-parameters-a-new-framework-for-neural-network-training","text":"The previous approaches rely on the process of fitting an object representation to noisy data by using regularized minimization methods (either by using the sparsity as regularizer, like in the CS approach; or using the output of D in the GAN reconstruction). However, a machine-learning step-forward is to estimate a direct mapping, F(m; \u03b8) , parametrized by \u03b8 , from the interferometric data, m , to the reconstructed image, t , without pre-defining strong hyperparameters. In the NN framework, to find \u03b8 this approach requires (i) to learn the set of weights of the neuron connections during the training process and (ii) to properly define the activation functions of each one of the neurons, when designing the network architecture. One of the key aspects of pattern recognition from NNs is the role of the activation function. The activation function decides, whether a neuron should be activated or not with the purpose of introducing non-linearity into the output of a neuron. Determining the different activation functions in the architecture of the network is essential for its correct functioning. There are, at least, three types of activation functions: saturated, unsaturated and adaptive. Saturated activation functions are used for decision boundaries (e.g., Sigmoid). However, this type of activation functions could lead to vanishing gradient problems during the training process. Unsaturated activation functions solve the vanishing gradient problem but, it they fall into a negative region, is highly improbable that the neuron could be activated again (e.g., Rectified Linear Unit -ReLU-). This could be solved by adding a constant term that acts like threshold for the negative part of the function (e.g., Leaky ReLU). Nevertheless, including this new pre-defined constant is like adding an hyperparameter which could strongly affect the learning process of the NN. Adaptive activation functions solve these problems by using trainable coefficients. The use of adaptive activations functions has demonstrated to have interesting implications in making the learning process faster and more effective. Figure 1. Schematics of a Generative Adversarial Network (GAN). This type of neural network confronts two nets in a minmax game. The first one is the generator (G) which is going to create new samples of a signal from a compressed dimensional space (i.e., the latent space). The second one is the discriminator (D), which tries to determine whether the input from G is fake in comparison with a sample or real images.","title":"2.2. Adaptive hyper-parameters, a new framework for Neural Network training"},{"location":"NN/#3-example-of-nn-imaging","text":"","title":"3. Example of NN imaging"},{"location":"NN/#31-observations-and-data","text":"The data added to illustrate our NN reconstruction approach correspond to Sparse Aperture Masking (SAM) observations were taken with the NACO-VLT infrared camera. Particularly, we recorded SAM data on the extended infrared source GC IRS 1W in the Galactic Center (GC). This target is a previously identified stellar bow-shock source produced by the relative motion of the central star and the momentum balance shock between the stellar wind and the interstellar medium. The observations were performed using the L27 camera (0.027\u201d/pixel) with the L\u2019 broad-band filter (\u03bb0 = 3.80 \u03bcm, \u2206\u03bb = 0.62 \u03bcm) combined with the BB 9holes NRM. We obtained 4 different data cubes from which the interferometric observable where extracted using CASSINI-SAMPip . Fig. 2 displays the calibrated squared visibilities, closure phases and u-v plane. The file MERGED_IRS1W.oifits contains all the SAM data used in this example and it is included in the Github repository. Figure 1. The lef t panel shows the calibrated squared visibilities of GC IRS 1w versus spatial frequencies obtained from our NACO-SAM observations. Similarly, the middle panel shows the calibrated closure phases versus spatial frequencies. The right panel displays the u-v plane of the observations.","title":"3.1. Observations and Data"},{"location":"NN/#32-software-architecture","text":"Our NN algorithm is based on the AUTOMAP architecture. The AUTOMAP network has been used to reconstruct images from medical data obtained through Magnetic Resonance Imaging (MRI). This network has demonstrated to be quite efficient in mapping between sensor and image domains with the use of appropriate training data. The network presented here, called CASSINI-AUTOMAP , follows this approach and it maps directly the interferometric data to the reconstructed image by using a convolutional neural network. Figure 3 displays the model summary with the different layers used and the number of parameters trained on each one. Figure 4 shows a schematic representation of our NN. In the diagram, we can observe the two main sections of the network. The first one corresponds to three fully connected layers (FC1, FC2 and FC3) which map the input observables with the number of pixels in the output image. FC3 has a dimension equivalent to n2 \u00d7 1, where n is the number of pixels per side in the output image, in our case n=64. FC1 uses a simple sigmoid activation function. FC2 and FC3 use an adaptive hyperbolic tangent (Atanh) activation function with two trainable parameters. The second section employs three convolutional layers (C1, C2 and C3). C1 and C2 convolve 64 filters of 5 \u00d7 5 kernel size with stride 1 while, C3 uses a 7 \u00d7 7 kernel size. C1 and C2 employ a parametric rectified linear unit (PReLU) activation function and dropout with a probability of 0.1 before being connected between them. C3 and the output layer, C , are used to deconvolve the image into a 64 \u00d7 64 array. Figure 3. The figure summarizes the different layers used in our CASSINI-AUTOMAP network. The first column displays the name of the different layers; the second column shows the output shape at a given layer and; the third column lists the number of trainable parameters at each layer. Figure 4. Schematic of the CASSINI-AUTOMAP network. The 1D and 2D sections are displayed with different colors. The output of the network correspond to a 64 \u00d7 64 image of a stellar bow shock. Our network was deployed in Python using the Keras module in an Apple Mac BookPro computer under the PlaidML environment. The training of the network was performed using an external AMD Radeon RX 570 GPU with 4 GB of memory. CASSINI-AUTOMAP is accessible in the Github repository of this documentation. It is a single script that can be run with the following command: >> python AUTOMAP_keras_4.py --mode train --epochs 300 --batch-size 128 IMPORTANT: before running the previous command, it is neccessary to have the following two files: xnew_OBTot.npz and bs64_flux.npz in the sub-directory NN_imaging of CASSINI . Due to limitations in the size of the files uploaded to Github, those files can be retrieved from this Dropbox folder . As it can be observed AUTOMAP runs in-line on the Terminal with the following available commands: 1. --mode (train, generate, validate) 2. --batch_size (integer value) 3. --nice (True, False) 4. --epoch (integer value) 5. --oifilename (filename of the oifits data when recover interferometric images from data) As part of the repository a .npz file is included with 2000 compressed radiative transfer models that can be used to train the network. These models are acompained with their corresponding interferometric observables. Both files are hardcoded in the NN code, they are asigned to the variables: load_data = np.load('bs64_flux.npz') ## The RT models load_oidata = np.load('xnew_OBTot.npz') ## The interferometric observables from the RT models This setup will allow us to fully train the network over 300 epochs using the Adam optimizer. From our tests, we notice that the number of epochs used is, at least, a third of the ones required without using adaptive activation functions. This results in a significant reduction of the processing time required during training. The cost function used for back-propagation of the gradient is the regression mean-squared error (MSE) loss. Figure 5 displays the MSE obtained from the different batches per epoch over the training process. The evolution of the MSE converges after 300 epochs. The biggest jump is observed around the first 20 epochs. This coincides with the epoch when the network learns the general shape of the bow shock. Figure 5. Evolution of the cost function (mean-square error) over the 300 epochs in which the CASSINI-AUTOMAP network was trained. Every 50 epochs the code saves the state of the weights in the network. In case the user needs to stop the net for debugging process. Every ten epochs the code also saves a .png file with a random series of batch images with the state of the trainning process; the code also saves a .fits file with those images for further inspection by the user. Fig. 6 displays an example of the images generated. Figure 6. The image displays different panels obtained during the training process of AUTOMAP. Different morphologies of the bowshocks are recovered (i.e., different sizes, position angles, inclinations, brigthness, etc). To illustrate better how the trainning process work, we plotted in Fig. 7 a sample of 5 different images obtained at different epochs, that clearly illustrate the evolution of the training process, at Epoch 0 there is only noise in the predicted images. However, as the network weights are evolving, central elongated structures appear and gradually the form of bow shocks emerge. Figure 7. The image displays 25 panels with reconstructed images obtained with CASSINI-AUTOMAP. Every column shows 5 images at different epochs. Each panel covers an equivalent area of 640 \u00d7 640 mas. The emission is normalized to the peak of each image.","title":"3.2. Software architecture"},{"location":"NN/#33-reconstruction-and-benckmarking","text":"Once the trainning process is finished (it is important that the user validates - this is done with the --mode validate keyword - the trainning of the net using independent models), AUTOMAP could be used to recover an image with real data. to do this, the user should include the following instruction in the Terminal: >> python AUTOMAP_keras_4.py --mode generate --oifilename MERGED_IRS1W.oifits where MERGED_IRS1W.oifits is the OIFITS file with the calibrated data that we are using for the reconstruction. For this example, we predict the morphology of the GC IRS 1W bow shock by randomly creating 100 samples from the interferometric data, assuming Gaussian distributions based on the mean and standard deviation of each observable. The mean image obtained with CASSINI-AUTOMAP was able to recover the interferometric observables with great accuracy. Indeed, the morphology of the bow-shock is clearly visible, even for pixel values below 5% of the peak value. We estimated a SNR \u223c 40 for pixel values of 5% of the emission peak. In order to benchmark our neural network imaging algorithm, images from the SAM data were reconstructed using the regularized minimization algorithm BSMEM (21; 22). This code uses entropy as regularizer to encode the prior information of the source\u2019s brightness distribution. This code uses the squared visibilities, closure phases and closure amplitudes to fit the model images with the data. Images were recovered using a pixel scale of 10 mas/px over a pixel grid of 64 \u00d7 64 px. Fig. 8 displays the best reconstructed image using BSMEM (the code converges to a \u03c72 = 1.1). Notice how the structure of the bow shock is recovered. The apex and the tails of the morphology are clearly observed. It can be seen that the peak of the brightness distribution is over the tails of the bow shock. The synthetic observables from the reconstructed images are displayed on top of the interferometric observables from the data. The best-fit image appears to recover quite well the trend observed in the interferometric data. The reconstructed image with the NN (Fig. 9) also exhibits an asymmetric bow shock with the peak of the emission over the tails of the shock front. The estimated size of the bow shock and the position angle of the apex (PA \u223c 25\u00ba) are similar between the BSMEM image and the CASSINI-AUTOMAP one. However, we could clearly identify that the emission below 10% of the peak value in the BSMEM image is very asymmetric, while, in the NN image the emission follows the same structure of the tails. This effect is caused because of the models employed for training the network. However, this does not compromise the fit of the observables. Adding more structures (like point-like sources) to the models used for training could help us to discover additional asymmetries in the source structure. The interesting idea behind this is the possibility of linking these additional sources with well-recognized physical structures, for example dust clumps or stellar companions. Figure 8. Left panel: Normalized BSMEM reconstructed image from our SAM data. The white contours show the 5, 10, 30, 50, 70 and 90 % of the source\u2019s peak. Middle and right panels: Squared visibilities and closure phases versus spatial frequencies. The NACO-SAM data is displayed with black dots and the observables extracted from the reconstructed image are displayed with red dots. Figure 9. Left panel: Normalized CASSINI-AUTOMAP reconstructed image from our SAM data. The white contours show the 5, 10, 30, 50, 70 and 90 % of the source\u2019s peak. Middle and right panels: Squared visibilities and closure phases versus spatial frequencies. The NACO-SAM data is displayed with black dots and the observables extracted from the reconstructed image are displayed with red dots.","title":"3.3 Reconstruction and benckmarking"},{"location":"Publications/","text":"Publications in peer-reviewed journals (Q1) associated with this project The study of accretion disk's morphology: Four papers were published on this topic. The first one details the variability of the inner edge of the accretion disk around HD 163296. We use parametric component modeling and chromatic image reconstruction. In the second paper we demonstrate the possibility of recovering images and velocity maps along the BrG spectral line in the object HD 58647. In the third paper, SAM-type data were used to search for sub-stellar companions in the inner cavities of accretion disks in a small sample of objects. In the fourth paper, we studied asymmetries in the accretion disk of LkCa 15 using SAM-type data. Publication 1 - Searching for low-mass companions at small separations in transition disks with aperture masking interferometry Publication 2 - The GRAVITY young stellar object survey: XI. Imaging the hot gas emission around the Herbig Ae star HD 58647 Publication 3 - Two Rings and a Marginally Resolved, 5 au Disk around LkCa 15 Identified via Near-infrared Sparse Aperture Masking Interferometry Publication 4 - The GRAVITY young stellar object survey. VI. Mapping the variable inner disk of HD 163296 at sub-au scales The study of the structure of oxygen-rich AGB stars: One paper was published and another is under review on this topic. The existence of clumpy and irregular CO layers in which dust grains of magnesium-based compounds can potentially coagulate was demonstrated. In the second article, the photosphere of the star R Car was resolved, in which bright spots associated with convective cells can be distinguished. The reconstructed dynamical images have helped us to distinguish changes in these convective patterns. This is the first work reporting these changes in AGB-type stars. In addition, we collaborated in the study of the dust distribution of the star Betelgeuse with the MATISSE instrument of the VLTI. Publication 5 - Imaging the innermost gaseous layers of the Mira star R Car with GRAVITY-VLTI Publication 6 - The dusty circumstellar environment of Betelgeuse during the Great Dimming as seen by VLTI/MATISSE The characterization of the effect of multiplicity in massive stars: The first images of the dust generated by the stellar wind collision of the binaries WR 137 and WR 140 were produced. The images of WR 137 are the first Fizeau-type interferometric reconstructions obtained with a space telescope. CASSINI-SAMPip reduction software was essential in the publication of this work. Additionally, a paper was published on the chromatic changes of the cavity produced in the collision region of the winds of the Eta Car binary. For this purpose, data with the MATISSE instrument of the VLTI in the mid-infrared were used. Publication 7 - A First Look with JWST Aperture Masking Interferometry (AMI): Resolving Circumstellar Dust around the Wolf-Rayet Binary WR 137 beyond the Rayleigh Limit Publication 8 - Nested dust shells around the Wolf-Rayet binary WR 140 observed with JWST Publication 9 - VLTI-MATISSE chromatic aperture-synthesis imaging of \u03b7 Carinae's stellar wind across the Br\u03b1 line. Periastron passage observations in February 2020 Papers on the observing modes of the JWST NIRISS instrument: Two papers were published on the \"Sparse Aperture Masking\" and \"Kernel Phases\" observing modes. Both in collaboration with the Space Telescope Science Institute science team responsible for both observing modes for the NIRISS near-infrared camera. Publication 10 - The Near Infrared Imager and Slitless Spectrograph for the James Webb Space Telescope. IV. Aperture Masking Interferometry Publication 11 - The Near Infrared Imager and Slitless Spectrograph for JWST. V. Kernel Phase Imaging and Data Analysis Research on the central structure of AGNs: A paper was published on the structure of the dust torus around the supermassive black hole in the active galaxy Circinus. Mid-infrared data obtained with MATISSE-VLTI and the VISIR-VLT camera were used. The VISIR data were reduced with CASSINI-SAMPip. Publication 12 - The dusty heart of Circinus. I. Imaging the circumnuclear dust in N-band","title":"Publications"},{"location":"Publications/#publications-in-peer-reviewed-journals-q1-associated-with-this-project","text":"The study of accretion disk's morphology: Four papers were published on this topic. The first one details the variability of the inner edge of the accretion disk around HD 163296. We use parametric component modeling and chromatic image reconstruction. In the second paper we demonstrate the possibility of recovering images and velocity maps along the BrG spectral line in the object HD 58647. In the third paper, SAM-type data were used to search for sub-stellar companions in the inner cavities of accretion disks in a small sample of objects. In the fourth paper, we studied asymmetries in the accretion disk of LkCa 15 using SAM-type data.","title":"Publications in peer-reviewed journals (Q1) associated with this project"},{"location":"Publications/#publication-1-searching-for-low-mass-companions-at-small-separations-in-transition-disks-with-aperture-masking-interferometry","text":"","title":"Publication 1 - Searching for low-mass companions at small separations in transition disks with aperture masking interferometry"},{"location":"Publications/#publication-2-the-gravity-young-stellar-object-survey-xi-imaging-the-hot-gas-emission-around-the-herbig-ae-star-hd-58647","text":"","title":"Publication 2 - The GRAVITY young stellar object survey: XI. Imaging the hot gas emission around the Herbig Ae star HD 58647"},{"location":"Publications/#publication-3-two-rings-and-a-marginally-resolved-5-au-disk-around-lkca-15-identified-via-near-infrared-sparse-aperture-masking-interferometry","text":"","title":"Publication 3 - Two Rings and a Marginally Resolved, 5 au Disk around LkCa 15 Identified via Near-infrared Sparse Aperture Masking Interferometry"},{"location":"Publications/#publication-4-the-gravity-young-stellar-object-survey-vi-mapping-the-variable-inner-disk-of-hd-163296-at-sub-au-scales","text":"The study of the structure of oxygen-rich AGB stars: One paper was published and another is under review on this topic. The existence of clumpy and irregular CO layers in which dust grains of magnesium-based compounds can potentially coagulate was demonstrated. In the second article, the photosphere of the star R Car was resolved, in which bright spots associated with convective cells can be distinguished. The reconstructed dynamical images have helped us to distinguish changes in these convective patterns. This is the first work reporting these changes in AGB-type stars. In addition, we collaborated in the study of the dust distribution of the star Betelgeuse with the MATISSE instrument of the VLTI.","title":"Publication 4 - The GRAVITY young stellar object survey. VI. Mapping the variable inner disk of HD 163296 at sub-au scales"},{"location":"Publications/#publication-5-imaging-the-innermost-gaseous-layers-of-the-mira-star-r-car-with-gravity-vlti","text":"","title":"Publication 5 - Imaging the innermost gaseous layers of the Mira star R Car with GRAVITY-VLTI"},{"location":"Publications/#publication-6-the-dusty-circumstellar-environment-of-betelgeuse-during-the-great-dimming-as-seen-by-vltimatisse","text":"The characterization of the effect of multiplicity in massive stars: The first images of the dust generated by the stellar wind collision of the binaries WR 137 and WR 140 were produced. The images of WR 137 are the first Fizeau-type interferometric reconstructions obtained with a space telescope. CASSINI-SAMPip reduction software was essential in the publication of this work. Additionally, a paper was published on the chromatic changes of the cavity produced in the collision region of the winds of the Eta Car binary. For this purpose, data with the MATISSE instrument of the VLTI in the mid-infrared were used.","title":"Publication 6 - The dusty circumstellar environment of Betelgeuse during the Great Dimming as seen by VLTI/MATISSE"},{"location":"Publications/#publication-7-a-first-look-with-jwst-aperture-masking-interferometry-ami-resolving-circumstellar-dust-around-the-wolf-rayet-binary-wr-137-beyond-the-rayleigh-limit","text":"","title":"Publication 7 - A First Look with JWST Aperture Masking Interferometry (AMI): Resolving Circumstellar Dust around the Wolf-Rayet Binary WR 137 beyond the Rayleigh Limit"},{"location":"Publications/#publication-8-nested-dust-shells-around-the-wolf-rayet-binary-wr-140-observed-with-jwst","text":"","title":"Publication 8 - Nested dust shells around the Wolf-Rayet binary WR 140 observed with JWST"},{"location":"Publications/#publication-9-vlti-matisse-chromatic-aperture-synthesis-imaging-of-carinaes-stellar-wind-across-the-br-line-periastron-passage-observations-in-february-2020","text":"Papers on the observing modes of the JWST NIRISS instrument: Two papers were published on the \"Sparse Aperture Masking\" and \"Kernel Phases\" observing modes. Both in collaboration with the Space Telescope Science Institute science team responsible for both observing modes for the NIRISS near-infrared camera.","title":"Publication 9 - VLTI-MATISSE chromatic aperture-synthesis imaging of \u03b7 Carinae's stellar wind across the Br\u03b1 line. Periastron passage observations in February 2020"},{"location":"Publications/#publication-10-the-near-infrared-imager-and-slitless-spectrograph-for-the-james-webb-space-telescope-iv-aperture-masking-interferometry","text":"","title":"Publication 10 - The Near Infrared Imager and Slitless Spectrograph for the James Webb Space Telescope. IV. Aperture Masking Interferometry"},{"location":"Publications/#publication-11-the-near-infrared-imager-and-slitless-spectrograph-for-jwst-v-kernel-phase-imaging-and-data-analysis","text":"Research on the central structure of AGNs: A paper was published on the structure of the dust torus around the supermassive black hole in the active galaxy Circinus. Mid-infrared data obtained with MATISSE-VLTI and the VISIR-VLT camera were used. The VISIR data were reduced with CASSINI-SAMPip.","title":"Publication 11 - The Near Infrared Imager and Slitless Spectrograph for JWST. V. Kernel Phase Imaging and Data Analysis"},{"location":"Publications/#publication-12-the-dusty-heart-of-circinus-i-imaging-the-circumnuclear-dust-in-n-band","text":"","title":"Publication 12 - The dusty heart of Circinus. I. Imaging the circumnuclear dust in N-band"},{"location":"RCAR/","text":"This repository includes software to analyze the GRAVITY-VLTI data of the AGB Mira star R Car. Link to the GitHub repository with the code, click HERE 1. RCar - GRAVITY This section contains a series of scripts (in the form of a Jupyter notebook) used for analyzing GRAVITY-VLTI data of the M-type AGB star R Car (see Rosales-Guzman et al. (2023) ). The Jupyter notebook is divided into the following 3 parts. To run the codes, just download the repository and open the Jupyter notebook RCar_analysis.ipynb to star working with it using the following command: >> user@users_pc ~ % Jupyter notebook RCar_analysis.ipynb Required packages (Python > 3.0): Astropy emcee lmfit matplotlib numpy Open CV scipy sklearn oitools.py (provided) MCMC fit to the V 2 across the pseudo continuum To obtain the angular size of R Car across the K-band, we applied a geometrical model of a uniform disk (UD) to the V 2 data. The visibility function of our models is given by the following equation ( Berger & Segransan (2007) ): \\[ V_{\\mathrm{UD}}(u,v) = 2(\\mathrm{F_r})\\frac{J_1(\\pi \\rho \\Theta_\\mathrm{UD} )}{\\pi \\rho \\Theta_{\\mathrm{UD}}}\\ \\] where \\(\\rho = \\sqrt{u^2+ v^2}\\) , u and v are the spatial frequencies sampled by the interferometric observations, J 1 is the first order Bessel function, \u0398 UD and F r are the angular diameter of the uniform disk profile and a scaling factor that accounts for the over-resolved flux in the observations, respectively. To fit the data, we used a Monte-Carlo Markov-Chain (MCMC) algorithm based on the Python package emcee ( Foreman et al., 2013 ). We let 250 walkers evolve for 150 steps using the data of each spectral bin independently. The results of this code are shown as three plots (Figs. 1, 2, and 3). Figure 1. Best-fit to the $V^2$ from the MCMC. The data are shown with red dots and the model with the blue line. The values corresponding to the best model are also shown in the plot. Figure 2. Positions of each walker as a function of the number of steps in the chain. Figure 3. The corner plot shows all the one and two dimensional projections of the posterior probability distributions of our parameters D (or $\\Theta_{UD}$) and $F_r$. The marginalized distribution for each parameter is shown in the histograms along the diagonal. The marginalized two dimensional distribution is also plotted. Single-layer model to fit the \\(V^2\\) across the first and second CO bandheads The MOLsphere model, used to calculate the size, temperature, and optical thickness of the CO layer consists of a stellar disk with a compact layer around it. The star is modeled by a stellar surface of radius \\(R_*\\) which emits as a black-body at a temperature \\(\\mathrm{T_{*}}\\) . It is surrounded by a compact spherical layer of radius \\(\\mathrm{R_{L}}\\) that absorbs the radiation emitted by the star and re-emits it like a black-body. The MOLsphere is characterized by its temperature \\(\\mathrm{T_{L}}\\) , radius \\(\\mathrm{R_L}\\) and its optical depth \\(\\tau_{\\lambda}\\) . The region between the stellar photosphere and the layer is assumed to be empty (See the figure for a skecth of the model). The analytical expression of the model is given by: where \\(I_\\lambda^r = I_{\\lambda}^r(\\mathrm{T_{*}} , \\mathrm{T_L}, \\mathrm{R_*}, \\mathrm{R_L}, \\tau_{\\lambda})\\) , \\(\\mathrm{T_*}\\) and \\(\\mathrm{T_L}\\) are the temperatures of the photosphere and of the CO layer, respectively; \\(\\mathrm{R_*}\\) and \\(\\mathrm{R_L}\\) are the angular radius of the star and the layer, respectively; \\(\\tau_\\lambda\\) is the optical depth of the molecular layer at wavelength \\(\\lambda\\) ; \\(B_\\lambda(\\mathrm{T})\\) is the Planck function (at wavelength \\(\\lambda\\) and temperature T); and \\(\\beta\\) is the angle between the radius vector and the line-of-sight so that \\(\\cos\\beta = \\sqrt{1-(\\mathrm{r}/\\mathrm{R_L})^2}\\) . For the fitting, we adopted a \\(\\mathrm{T_{*}}\\) =2800 K and \\(\\mathrm{R_{*}}\\) =5 mas ( Monnier et al., 2014 ; McDonald et al., 2012 ). We use the disk estimation reported in the H- band because it is considerably less affected from molecular contribution, in contrast with the K-band where molecules like CO, \\(H_{2}O\\) , and OH among others (see e.g. Wittkowski et al., 2018 ; Paladini 2011 ) are present over the entire band. The unknown parameters are then \\(\\mathrm{T_L}\\) , \\(\\mathrm{R_L}\\) and \\(\\tau_L\\) . We performed a two-step process to estimate these parameters. As first step, for each pair of \\(\\mathrm{T_L}\\) and \\(\\mathrm{R_L}\\) , we performed a least-squares minimization to find the best-fit value of \\(\\tau_L\\) that reproduces the spectrum \\(F_{\\lambda}^{\\mathrm{R Car}}\\) . The next step consisted of using an MCMC method based on the Python library emcee to estimate the best combinations of \\(\\mathrm{T_L}\\) , \\(\\mathrm{R_L}\\) , and \\(\\tau_L\\) that reproduce the \\(V^2\\) . To account for the over-resolved flux when modeling the \\(V^2\\) data, we added a scaling factor Fr. This value was estimated simultaneously with the other parameters in the model. Figure 4. Illustration of the single-layer model. The yellow disk represents the star, and the orange ring represents the layer. The parameters in the image are as described in the text above. Figs. 5 and 6 are generated by the single-layer model part of the notebook. Figure 5. Best-fit to the V 2 using the Single-layer model. The green dots correspond to the data and the purple ones to the fit (see the labels on the plot). The best-fit parameters are shown in the top right corner of the figure. Figure 6. Corner plot of all the one and two dimensional projections of the posterior probability distributions of our parameters T L , R L and F r . As shown in Fig. 3, the marginalized distribution for each parameter is located in the histograms along the diagonal. 2. Bootstrapp method for image reconstruction To build new data samples from the original one, we employed the bootstrap technique. The samples were created by keeping the original number of data points unaletered, but allowwing random sampling with replacement. This created some data sets where some points have higher weights than in the original data set, and some others with zero weights. This algorithm also produces slight changes in the u-v plane, which allows us to trace the impact of the u-v coverage on the reconstructed images, without loosing the statistical significance of the original number of data points (see Fig. 7). According to Babu & Singh 1983 , the statistical moments such as mean, variance and standard deviation of the bootstrapped samples are good approximations to the statistical moments of the original data sets. Finally, since each bootstrapped data set is different, we will have slightly different reconstructed images. Figure 7. From left to right: $V^2$ and CPs as a function of the Spatial Frequency, and UV coverage for a sample of ten bootstrapped data sets. Each color corresponds to a different bootstrapped data set. 3. Principal Component Analysis (PCA) to understand the structure of R Car To have a more precise characterization of the asymmetries in the reconstructed images of the CO band-heads, we used the Principal Component Analysis (PCA) described by Medeiros et al., 2018 . Those authors demonstrate that the visibilities of the Principal Components are equal to the Principal Components of the visibilities. This method is useful to trace the changes across a set of images that have the largest effect on the visibility (amplitude and -closure- phase) profile. In our case, we estimate the most significant structural changes of the observed asymmetric structures across wavelength for each of the observed epochs. The following procedure was applied to the ensemble of wavelength dependent images per epoch to extract their Principal Components. For the PCA analysis, we normalize each data set by subtracting the corresponding mean image and dividing by their standard deviation image across the wavelength range. To perform the PCA analysis, we used the CASSINI-PCA package. This software allows to compute the covariance matrix of the data set and to transform it into the space of the principal components. This allows us to determine the eigenvectors (or eigen-images, in our case) and their corresponding eigenvalues. Since we only have seven images per data set, and the possible number of components must be smaller or equal than the number of images in the data set, we decided to only keep the first four principal components. From our tests, we observed that those components explain, at least, the 93\\% of the variance in the data sets. Figure 8. First four Principal Components for the January epoch for the 1st CO band head. The relative scale of the structures in the eigen-images is displayed with a colorbar at the bottom of each panel. Only the central 20 mas of the eigen-images are shown on each panel. The white contours correspond to the mean image across wavelength (per given data set) and they represent 10, 30, 50, 70, 90, 95, 97, and 99 % of the intensity's peak. Figure 9. Representative example of the CPs versus spatial frequencies of the data set that corresponds to the 1st CO band-head at 2.2946 \u03bcm (gray squares). The colored dots show the CPs from the recovered images obtained with different numbers of Principal Components (see label on the plot). Disclaimer All the scripts that compose this repository are part of the A&A article Imaging the innermost gaseous layers of the Mira star R Car with GRAVITY-VLTI from Rosales-Guzman et al. For enquiries and/or contributions please contact jarosales@astro.unam.mx, who is a PhD student associated with this project.","title":"GRAVITY - RCAR analysis"},{"location":"RCAR/#this-repository-includes-software-to-analyze-the-gravity-vlti-data-of-the-agb-mira-star-r-car","text":"","title":"This repository includes software to analyze the GRAVITY-VLTI data of the AGB Mira star R Car."},{"location":"RCAR/#link-to-the-github-repository-with-the-code-click-here","text":"","title":"Link to the GitHub repository with the code, click HERE"},{"location":"RCAR/#1-rcar-gravity","text":"This section contains a series of scripts (in the form of a Jupyter notebook) used for analyzing GRAVITY-VLTI data of the M-type AGB star R Car (see Rosales-Guzman et al. (2023) ). The Jupyter notebook is divided into the following 3 parts. To run the codes, just download the repository and open the Jupyter notebook RCar_analysis.ipynb to star working with it using the following command: >> user@users_pc ~ % Jupyter notebook RCar_analysis.ipynb","title":"1. RCar - GRAVITY"},{"location":"RCAR/#required-packages-python-30","text":"Astropy emcee lmfit matplotlib numpy Open CV scipy sklearn oitools.py (provided)","title":"Required packages (Python &gt; 3.0):"},{"location":"RCAR/#mcmc-fit-to-the-v2-across-the-pseudo-continuum","text":"To obtain the angular size of R Car across the K-band, we applied a geometrical model of a uniform disk (UD) to the V 2 data. The visibility function of our models is given by the following equation ( Berger & Segransan (2007) ): \\[ V_{\\mathrm{UD}}(u,v) = 2(\\mathrm{F_r})\\frac{J_1(\\pi \\rho \\Theta_\\mathrm{UD} )}{\\pi \\rho \\Theta_{\\mathrm{UD}}}\\ \\] where \\(\\rho = \\sqrt{u^2+ v^2}\\) , u and v are the spatial frequencies sampled by the interferometric observations, J 1 is the first order Bessel function, \u0398 UD and F r are the angular diameter of the uniform disk profile and a scaling factor that accounts for the over-resolved flux in the observations, respectively. To fit the data, we used a Monte-Carlo Markov-Chain (MCMC) algorithm based on the Python package emcee ( Foreman et al., 2013 ). We let 250 walkers evolve for 150 steps using the data of each spectral bin independently. The results of this code are shown as three plots (Figs. 1, 2, and 3). Figure 1. Best-fit to the $V^2$ from the MCMC. The data are shown with red dots and the model with the blue line. The values corresponding to the best model are also shown in the plot. Figure 2. Positions of each walker as a function of the number of steps in the chain. Figure 3. The corner plot shows all the one and two dimensional projections of the posterior probability distributions of our parameters D (or $\\Theta_{UD}$) and $F_r$. The marginalized distribution for each parameter is shown in the histograms along the diagonal. The marginalized two dimensional distribution is also plotted.","title":"MCMC fit to the V2 across the pseudo continuum"},{"location":"RCAR/#single-layer-model-to-fit-the-v2-across-the-first-and-second-co-bandheads","text":"The MOLsphere model, used to calculate the size, temperature, and optical thickness of the CO layer consists of a stellar disk with a compact layer around it. The star is modeled by a stellar surface of radius \\(R_*\\) which emits as a black-body at a temperature \\(\\mathrm{T_{*}}\\) . It is surrounded by a compact spherical layer of radius \\(\\mathrm{R_{L}}\\) that absorbs the radiation emitted by the star and re-emits it like a black-body. The MOLsphere is characterized by its temperature \\(\\mathrm{T_{L}}\\) , radius \\(\\mathrm{R_L}\\) and its optical depth \\(\\tau_{\\lambda}\\) . The region between the stellar photosphere and the layer is assumed to be empty (See the figure for a skecth of the model). The analytical expression of the model is given by: where \\(I_\\lambda^r = I_{\\lambda}^r(\\mathrm{T_{*}} , \\mathrm{T_L}, \\mathrm{R_*}, \\mathrm{R_L}, \\tau_{\\lambda})\\) , \\(\\mathrm{T_*}\\) and \\(\\mathrm{T_L}\\) are the temperatures of the photosphere and of the CO layer, respectively; \\(\\mathrm{R_*}\\) and \\(\\mathrm{R_L}\\) are the angular radius of the star and the layer, respectively; \\(\\tau_\\lambda\\) is the optical depth of the molecular layer at wavelength \\(\\lambda\\) ; \\(B_\\lambda(\\mathrm{T})\\) is the Planck function (at wavelength \\(\\lambda\\) and temperature T); and \\(\\beta\\) is the angle between the radius vector and the line-of-sight so that \\(\\cos\\beta = \\sqrt{1-(\\mathrm{r}/\\mathrm{R_L})^2}\\) . For the fitting, we adopted a \\(\\mathrm{T_{*}}\\) =2800 K and \\(\\mathrm{R_{*}}\\) =5 mas ( Monnier et al., 2014 ; McDonald et al., 2012 ). We use the disk estimation reported in the H- band because it is considerably less affected from molecular contribution, in contrast with the K-band where molecules like CO, \\(H_{2}O\\) , and OH among others (see e.g. Wittkowski et al., 2018 ; Paladini 2011 ) are present over the entire band. The unknown parameters are then \\(\\mathrm{T_L}\\) , \\(\\mathrm{R_L}\\) and \\(\\tau_L\\) . We performed a two-step process to estimate these parameters. As first step, for each pair of \\(\\mathrm{T_L}\\) and \\(\\mathrm{R_L}\\) , we performed a least-squares minimization to find the best-fit value of \\(\\tau_L\\) that reproduces the spectrum \\(F_{\\lambda}^{\\mathrm{R Car}}\\) . The next step consisted of using an MCMC method based on the Python library emcee to estimate the best combinations of \\(\\mathrm{T_L}\\) , \\(\\mathrm{R_L}\\) , and \\(\\tau_L\\) that reproduce the \\(V^2\\) . To account for the over-resolved flux when modeling the \\(V^2\\) data, we added a scaling factor Fr. This value was estimated simultaneously with the other parameters in the model. Figure 4. Illustration of the single-layer model. The yellow disk represents the star, and the orange ring represents the layer. The parameters in the image are as described in the text above. Figs. 5 and 6 are generated by the single-layer model part of the notebook. Figure 5. Best-fit to the V 2 using the Single-layer model. The green dots correspond to the data and the purple ones to the fit (see the labels on the plot). The best-fit parameters are shown in the top right corner of the figure. Figure 6. Corner plot of all the one and two dimensional projections of the posterior probability distributions of our parameters T L , R L and F r . As shown in Fig. 3, the marginalized distribution for each parameter is located in the histograms along the diagonal.","title":"Single-layer model to fit the \\(V^2\\) across the first and second CO bandheads"},{"location":"RCAR/#2-bootstrapp-method-for-image-reconstruction","text":"To build new data samples from the original one, we employed the bootstrap technique. The samples were created by keeping the original number of data points unaletered, but allowwing random sampling with replacement. This created some data sets where some points have higher weights than in the original data set, and some others with zero weights. This algorithm also produces slight changes in the u-v plane, which allows us to trace the impact of the u-v coverage on the reconstructed images, without loosing the statistical significance of the original number of data points (see Fig. 7). According to Babu & Singh 1983 , the statistical moments such as mean, variance and standard deviation of the bootstrapped samples are good approximations to the statistical moments of the original data sets. Finally, since each bootstrapped data set is different, we will have slightly different reconstructed images. Figure 7. From left to right: $V^2$ and CPs as a function of the Spatial Frequency, and UV coverage for a sample of ten bootstrapped data sets. Each color corresponds to a different bootstrapped data set.","title":"2. Bootstrapp method for image reconstruction"},{"location":"RCAR/#3-principal-component-analysis-pca-to-understand-the-structure-of-r-car","text":"To have a more precise characterization of the asymmetries in the reconstructed images of the CO band-heads, we used the Principal Component Analysis (PCA) described by Medeiros et al., 2018 . Those authors demonstrate that the visibilities of the Principal Components are equal to the Principal Components of the visibilities. This method is useful to trace the changes across a set of images that have the largest effect on the visibility (amplitude and -closure- phase) profile. In our case, we estimate the most significant structural changes of the observed asymmetric structures across wavelength for each of the observed epochs. The following procedure was applied to the ensemble of wavelength dependent images per epoch to extract their Principal Components. For the PCA analysis, we normalize each data set by subtracting the corresponding mean image and dividing by their standard deviation image across the wavelength range. To perform the PCA analysis, we used the CASSINI-PCA package. This software allows to compute the covariance matrix of the data set and to transform it into the space of the principal components. This allows us to determine the eigenvectors (or eigen-images, in our case) and their corresponding eigenvalues. Since we only have seven images per data set, and the possible number of components must be smaller or equal than the number of images in the data set, we decided to only keep the first four principal components. From our tests, we observed that those components explain, at least, the 93\\% of the variance in the data sets. Figure 8. First four Principal Components for the January epoch for the 1st CO band head. The relative scale of the structures in the eigen-images is displayed with a colorbar at the bottom of each panel. Only the central 20 mas of the eigen-images are shown on each panel. The white contours correspond to the mean image across wavelength (per given data set) and they represent 10, 30, 50, 70, 90, 95, 97, and 99 % of the intensity's peak. Figure 9. Representative example of the CPs versus spatial frequencies of the data set that corresponds to the 1st CO band-head at 2.2946 \u03bcm (gray squares). The colored dots show the CPs from the recovered images obtained with different numbers of Principal Components (see label on the plot).","title":"3. Principal Component Analysis (PCA) to understand the structure of R Car"},{"location":"RCAR/#disclaimer","text":"All the scripts that compose this repository are part of the A&A article Imaging the innermost gaseous layers of the Mira star R Car with GRAVITY-VLTI from Rosales-Guzman et al. For enquiries and/or contributions please contact jarosales@astro.unam.mx, who is a PhD student associated with this project.","title":"Disclaimer"},{"location":"SAMpip/","text":"This repository includes software to reduce data obtained with Fizeau interferometry in the form of Sparse Aperture Masking. Link to the GitHub repository with the code, click HERE 1. SAMPip Sparse Aperture Masking Interferometry is a technique that transforms a unitary telescope into a Fizeau interferometry by placing a mask with a series of pin-holes in the pupil plane of the telescope. Each pair of pin-holes produces a non-redundant baseline that samples a specific spatial frequency in the Fourier domain (i.e., a specific angular scale). This technique has important advantages compared with regular imaging: (a) the mask acts as spatial filter, therefore, only structures between 0.5 \u03bb/B and 5 \u03bb/B are sampled; (b) the Point-Spread Function (PSF) is a very well defined interferogram that depends, at first order, of the mask geometry; (c) the best-resolution achieved with this technique is two times better than the standard diffraction limit regime of a full-pupil observation. Figure 1. Simplified diagram of the principal elements of Sparse Aperture Masking interferometry. The design of the 7 pin-hole mask of NIRISS-JWST is shown on the left. The rightmost panel displays the simulation of the fringe pattern of a point source at the detector of NIRISS-JWST. CASSINI-SAMpip is a module that recovers the interferometric observables (squared visibilities and closure phases) from an interferogram obtained with a non-redundant mask (NRM). SAMPip uses the geometry of the to decompose the interferogram into a series of fringe patterns produced by each pair of pin-holes in the mask. For example, in the case of a non-redundant mask with 7 pin-holes, there will be 21 baselines. Each of them is decomposed into cosine and sine factors. In this case, the model of the interferogram, thus, contains 42 parameters, plus one more that accounts for the difraction pattern of the pin-hole geometry and an offset that accounts for the zero point flux. Fig. 2 includes an example of the interferogram produced by the NRM on board of the instrument NIRISS at the James Webb Space Telescope . Figure 2. Example of the interferogram produced with the non-redundant mask at NIRISS-JWST (lower-left white circle). The image also shows five examples of individual fringe patterns produced by different baselines in the mask, plus the diffraction pattern of the pin-hole shape. SAMPip uses an expanssion of the following fringe expression (the cosine and sine factors are repeated per baseline, BL i , in the array) to obtain the total flux in the interferogram, I 0 , as well as the visibilities, V i , and phases, \u03c6 i . The term kx t depends on the spatial frequency sampled by the mask. The complete mathematical description of the model used by SAMPip is described in Sivaramakrishnan et al. (2023) . Equation 1. Fringe equation per baseline used to model the interferogram in SAMPip. In order to extract the interferometric observables, the pixel values from the interferogram, I t (k,V,\u03c6) are extracted and the right side of the fringe equation is solved using a matrix inversion. Once the visibilities and phases are obtained, the closure phases are calculated from them. Please, take into account that the extracted phases could be highly affected by atmospheric (in case of ground-based observations) and instrumental systematics. Closure Phases (CPs) (see Fig. 2) are computed by obtaining the argument of the complex triple product of visibilities obtained from a closed triangle of baselines. Closure phases are particularly important because they are observables that are free of telescope-dependent errors. This observable traces deviations from centro-symmetric morphologies. With the use of closure phases, we could recover interferometric images, albeit there is less phase information than amplitude values than with regular Fourier phases. Figure 3. Diagram of closure phase extraction. The measured phases, \u03c6 i,j , are biased by telescope dependent errors, \u03c6 err . The triple product removes them. Tipically, the SAM data are stored in cubes composed by a sequence of frames. The raw observables are, thus, extracted frame by frame and the final ones are obtained through a weigthed average. Please, notice that when the data cubes are obtained, it is necessary to take into account the rotation of the parallactic angle. This should not vary more than a few degrees. Otherwise, by averaging the frames in a data cube will blur the interferometric fringes. Also, the model used by SAMPip corrects for the attenuation of the fringes' contrast due to broad-band filters used during the observations. Finally, it is important to mention that, in addition to the source's observations it is also necessary to observe a point-like object to calibrate the trasnfer function of the instrument. 2. Example of the use of SAMPip with the JWST (User Manual) SAMPip is included in this repository. The software consist in a series of Python scripts to perform the data reduction. We included a test.py script to load a simulated data as example for the reduction. This script setups the code according to the instrument requests. To exemplify the use of SAMPip, we include as example the setup used to analyze the data of the massive binary WR 137. The observational sequence and the full data analysis are described in Lau et al. (2023) . To reproduce the interferometric observables from the example included in this repository, it is as simple as clone SAMPip from the GitHub repository with the following command: >> git clone https://github.com/cosmosz5/SAMPip.git Inside the repository, there is a script that setup all the required parameters to run SAMPip. The script is called test.py . To run this code, simply type in the Terminal: >> python test.py The parameters included in test.py are described below: ####### The tolowing parameters are necessary to run SAMPip ################## mask_filename = '7holes_jwst_mask_corr.txt' #A text file with the mask geometry wave = 4.817e-06 #Central wavelength of the observations (meters) bandwidth = 0.298e-06 #Bandwidth of the observations (meters) hole_size = 0.82 #in meters imsize = 80 # in pixels px_scalex = 65.3249 #X-dimenssion pixel scale in mas px_scaley = 65.7226 #Y-dimenssion pixel scale in mas hole_geom = 'HEXAGON' #HEXAGON for the JWST inst = 'JWST' arrname = 'DATA' ## This could be DATA or SIM. For the \"on-sky\" data use DATA rotation_angle = -0.36 ### In case the mask is not propely aligned with the position indicated in the manual oversample = 1.0 ## Number of times that you want to oversample the data. This is employed in simulations, not with real data. scale_factor = [0.996, 0.999] #Small pixel scale correction center_factor = [0,0] #Centroid position, in case alignment is needed Once the main parameters are setup, then, the main instructions are run to extract the observables. For this example, first the 3.8 \u03bcm filter is run (for both science target and calibrator) and then the observables for the 4.8 \u03bcm filter are obtained. The following lines extract the observables as described before: data_filename = 'data_WR137_bpfix_480.txt' source = 'WR137' sim_SAM.simSAM_PSF (data_filename, mask_filename, wave, bandwidth, hole_size, px_scalex, px_scaley, imsize, hole_geom, source, inst, \\ arrname, rotation_angle, oversample, scale_factor, center_factor, affine=False) data_filename = 'data_cal_bpfix_480.txt' source = 'Cal' sim_SAM.simSAM_PSF (data_filename, mask_filename, wave, bandwidth, hole_size, px_scalex, px_scaley, imsize, hole_geom, source, inst, \\ arrname, rotation_angle, oversample, scale_factor, center_factor, affine=False) ## Here, we change the filter and bandpass: wave = 3.82650365041922e-06 bandwidth = 0.205e-06 ## data_filename = 'data_WR137_bpfix_380.txt' source = 'WR137' sim_SAM.simSAM_PSF (data_filename, mask_filename, wave, bandwidth, hole_size, px_scalex, px_scaley, imsize, hole_geom, source, inst, \\ arrname, rotation_angle, oversample, scale_factor, center_factor, affine=False) data_filename = 'data_cal_bpfix_380.txt' source = 'Cal' sim_SAM.simSAM_PSF (data_filename, mask_filename, wave, bandwidth, hole_size, px_scalex, px_scaley, imsize, hole_geom, source, inst, \\ arrname, rotation_angle, oversample, scale_factor, center_factor, affine=False) The Github repository also contains the .txt files with the list of targets necessary for the reduction. As it was mentioned before, the format of the data are cubes with different frames that include the interferograms. Those files can be explored using the DS9 software . After running the test.py script, the code will generate a series of .fits files for quality check purposes, together with the final OIFITS files with the extracted observables. The list of files created by the extraction of the observables are the following ones: 1. CENTERED_*.fits files > These are the centered input files. SAMPip does a fine adjustment of the interferogram centroid in the middle of the pixel grid (this option is not yet available but the files are produced). 2. MODEL_interferogram_*.fits > These are the models of the interferograms produced by the code. They should be quite similar to the input data files. We can use the MODEL files to inspect (quickly) visually how good is our fringe modeling (i.e., visibility extraction) 3. MODEL_interferogram_windowed_*.fits > These are the models of the interferograms produced by the code but they are windowed. Only the valid pixels for the model extraction have values different from zero. 4. MODEL_residuals+*.fits > These .fits files show the residuals between our fringe model and the input data. 5. MTF.fits > This .fits file includes the Mutual Trsfer Funcion of the interferogram 6. PSF.fits > This .fits file includes the model of the interferogram produced solely by the geometry of the non-redundant mask 7. cube_bl.fits > This .fits file contains the cube of the different interference pattern produce by each pair of pin-holes in the non-redundant mask. 8. hexa.fits > This file contains the diffraction pattern of the pin-hole geometry 9. hexagon.fits > This file contains the geometry of the pin-hole mask 10. WINDDAT_*.fits > These files are the windowed input data sets used for the model extraction 11. SIM_DATA_uncalib.*.fits > These are the OFITIS files with the reduced data. They are in a standard OIFITS format and includes all the interferometric observables extracted with SAMPip. The code also produces a series of .png files to show the quality of the observables' extraction. Figs. 4 and 5 show the distribution values (from the frames in a data cube) of the squared visibilities and closure phases for the 3.8 \u03bcm filter for WR 137. The mean value, \u03c3 and the standard deviation, \u03c3 per baseline and triplet. Figure 4. Distribution of squared visibility values per baseline (21) for the NIRISS-JWST observations of WR 137 (filter F380M). Figure 5. Distribution of closure phases values per triplet (35) for the NIRISS-JWST observations of WR 137 (filter F380M). SAMPip also produces a plot with the uncalibrated squared visibilities and closure phases versus spatial frequency. Fig. 6 displays an example of those observables for WR 137. Notice how the precision in the data are quite small (sometimes even below the symbol size). This is because of the superb quality of the JWST data. Figure 6. Squared visibilities and Closure phases per spatial frequency of WR 137 (filter F380M) extracted with SAMPip. In order to check if the extraction of the observables is correct the user should check the intermediate .fits files produced by SAMPip. A quick way of deduce the quality of the observables' extraction is to look at the residuals plots to look for systematic trends. Also, one could check the MODEL_interferogram_*.fits file. If it is similar to the structure of the interferogram in the data (e.g., size, asymmetries, position, etc), it also indicates that the extraction is correct. Fig. 7 displays the data and model fits side by side. It can be easily checked that both of them exhibits the same structure. Figure 7. The left panel shows the interferogram of one of the frames in the JWST-NIRISS/SAM data cube. The right panel shows the model generated with SAMPip from which the observables are extracted. Notice thow the structure is quite similar in both panels. The scale si the same in both of them. There are a few pixels in the MODEL panel that differ from the DATA, this is because those are bad pixels filtered out by SAMPip during modeling. Once the RAW OIFITS data are obtained, it is necessary to calibrate them. For this, we use the script called calib_sam_obo.py . It contains a few instructions (see below) that calls the main calibration routine. The calibration process divide the raw science squared visibilities by the calibrator ones and subtract the calibrator's closure phases from the data ones. Fig. 8 displays the calibrated observables. #### Calibration routine script #### input_sci_path = '' # Path of the science OIFITS files input_cal_path = '' # Path of the calibrator OIFITS files calibrator_filename = 'cal_data_bpfix_380.txt' # .txt file with the Calibrator data files science_filename = 'sci_data_bpfix_380.txt' # .txt file with the Science data files cal_name = 'Cal_new' # Prefix of the calibrated OIFITS data [sc_files] = readcol.readcol(input_sci_path+science_filename, twod=False) [cal_files] = readcol.readcol(input_cal_path+calibrator_filename, twod=False) for i in range(len(sc_files)): sc_dat = sc_files[i] cal_dat = cal_files[0] delta = 1000.0 ### Hours calibrate_SAM_obo.calibrate_SAM(input_sci_path, input_cal_path, sc_dat, cal_dat, cal_name, delta) print('done') Figure 8. Calibrated squared visibilities and closure phases versus spatial frequency. Notice that the visibility values decrease with spatial frequency, showing that the target is resolved. The closure phases varies from -20 to 20 degrees, which indicates that the target is not centro-symmetric. The calibrated data can be used, thus, for imaging. Here, we just included the resulting the reconstructed images of WR 137. For comparison Fig. 9 shows the reconstructions obtained from different software and extractions obtained from different pipelines. SAMPip extractions performs similar to other pipelines that use different methods. The full scientific exploitation of the data and characterization of NIRISS-SAM are described in: Lau et al. (2023) , Kammerer et al. (2023) , and Sivaramakrishnan et al. (2023) Figure 9. Images of the morphology of WR 137. The different panels show different extraction of the observables with three pipelines, including SAMPip; and three different reconstructions. The quality of the images is comparable with the different extractions. The yellow ellipse at the bottom of the upper-rightmost panel correspond to the maxium resolution element of the interferometer. Sometimes different data sets are reduced. They can be combined with the oi_combine.fits python routine. It is a very simple script which call oi_merge.fits and js_oifits.fits . Those are two routines which reads the different data sets and write a combined OIFITS file with them. oi_merge.fits and js_oifits.fits are also included in the root directory of CASSINI/SAMPip . These two routines are also used by the during the data reduction. To combine the extracted OIFITS data, it is neccesary to create a .txt file with the oifits filenames to be combined. An example of the oi_combine.fits content is the following one: from readcol import * cd = True if cd == True: import oi_merge as oi_merge else: import oi_merge2 as oi_merges# import importlib importlib.reload(oi_merge) data = 'combine_data_tot.txt' ## Text file with the OIFITS files to be combined [files] = readcol(data, twod=False) merged = oi_merge.oi_merge(files) merged.write('COMB_JWST_SAM_tot.fits') ## Output: COMBINED OIFITS file","title":"SAMPip"},{"location":"SAMpip/#this-repository-includes-software-to-reduce-data-obtained-with-fizeau-interferometry-in-the-form-of-sparse-aperture-masking","text":"","title":"This repository includes software to reduce data obtained with Fizeau interferometry in the form of Sparse Aperture Masking."},{"location":"SAMpip/#link-to-the-github-repository-with-the-code-click-here","text":"","title":"Link to the GitHub repository with the code, click HERE"},{"location":"SAMpip/#1-sampip","text":"Sparse Aperture Masking Interferometry is a technique that transforms a unitary telescope into a Fizeau interferometry by placing a mask with a series of pin-holes in the pupil plane of the telescope. Each pair of pin-holes produces a non-redundant baseline that samples a specific spatial frequency in the Fourier domain (i.e., a specific angular scale). This technique has important advantages compared with regular imaging: (a) the mask acts as spatial filter, therefore, only structures between 0.5 \u03bb/B and 5 \u03bb/B are sampled; (b) the Point-Spread Function (PSF) is a very well defined interferogram that depends, at first order, of the mask geometry; (c) the best-resolution achieved with this technique is two times better than the standard diffraction limit regime of a full-pupil observation. Figure 1. Simplified diagram of the principal elements of Sparse Aperture Masking interferometry. The design of the 7 pin-hole mask of NIRISS-JWST is shown on the left. The rightmost panel displays the simulation of the fringe pattern of a point source at the detector of NIRISS-JWST. CASSINI-SAMpip is a module that recovers the interferometric observables (squared visibilities and closure phases) from an interferogram obtained with a non-redundant mask (NRM). SAMPip uses the geometry of the to decompose the interferogram into a series of fringe patterns produced by each pair of pin-holes in the mask. For example, in the case of a non-redundant mask with 7 pin-holes, there will be 21 baselines. Each of them is decomposed into cosine and sine factors. In this case, the model of the interferogram, thus, contains 42 parameters, plus one more that accounts for the difraction pattern of the pin-hole geometry and an offset that accounts for the zero point flux. Fig. 2 includes an example of the interferogram produced by the NRM on board of the instrument NIRISS at the James Webb Space Telescope . Figure 2. Example of the interferogram produced with the non-redundant mask at NIRISS-JWST (lower-left white circle). The image also shows five examples of individual fringe patterns produced by different baselines in the mask, plus the diffraction pattern of the pin-hole shape. SAMPip uses an expanssion of the following fringe expression (the cosine and sine factors are repeated per baseline, BL i , in the array) to obtain the total flux in the interferogram, I 0 , as well as the visibilities, V i , and phases, \u03c6 i . The term kx t depends on the spatial frequency sampled by the mask. The complete mathematical description of the model used by SAMPip is described in Sivaramakrishnan et al. (2023) . Equation 1. Fringe equation per baseline used to model the interferogram in SAMPip. In order to extract the interferometric observables, the pixel values from the interferogram, I t (k,V,\u03c6) are extracted and the right side of the fringe equation is solved using a matrix inversion. Once the visibilities and phases are obtained, the closure phases are calculated from them. Please, take into account that the extracted phases could be highly affected by atmospheric (in case of ground-based observations) and instrumental systematics. Closure Phases (CPs) (see Fig. 2) are computed by obtaining the argument of the complex triple product of visibilities obtained from a closed triangle of baselines. Closure phases are particularly important because they are observables that are free of telescope-dependent errors. This observable traces deviations from centro-symmetric morphologies. With the use of closure phases, we could recover interferometric images, albeit there is less phase information than amplitude values than with regular Fourier phases. Figure 3. Diagram of closure phase extraction. The measured phases, \u03c6 i,j , are biased by telescope dependent errors, \u03c6 err . The triple product removes them. Tipically, the SAM data are stored in cubes composed by a sequence of frames. The raw observables are, thus, extracted frame by frame and the final ones are obtained through a weigthed average. Please, notice that when the data cubes are obtained, it is necessary to take into account the rotation of the parallactic angle. This should not vary more than a few degrees. Otherwise, by averaging the frames in a data cube will blur the interferometric fringes. Also, the model used by SAMPip corrects for the attenuation of the fringes' contrast due to broad-band filters used during the observations. Finally, it is important to mention that, in addition to the source's observations it is also necessary to observe a point-like object to calibrate the trasnfer function of the instrument.","title":"1. SAMPip"},{"location":"SAMpip/#2-example-of-the-use-of-sampip-with-the-jwst-user-manual","text":"SAMPip is included in this repository. The software consist in a series of Python scripts to perform the data reduction. We included a test.py script to load a simulated data as example for the reduction. This script setups the code according to the instrument requests. To exemplify the use of SAMPip, we include as example the setup used to analyze the data of the massive binary WR 137. The observational sequence and the full data analysis are described in Lau et al. (2023) . To reproduce the interferometric observables from the example included in this repository, it is as simple as clone SAMPip from the GitHub repository with the following command: >> git clone https://github.com/cosmosz5/SAMPip.git Inside the repository, there is a script that setup all the required parameters to run SAMPip. The script is called test.py . To run this code, simply type in the Terminal: >> python test.py The parameters included in test.py are described below: ####### The tolowing parameters are necessary to run SAMPip ################## mask_filename = '7holes_jwst_mask_corr.txt' #A text file with the mask geometry wave = 4.817e-06 #Central wavelength of the observations (meters) bandwidth = 0.298e-06 #Bandwidth of the observations (meters) hole_size = 0.82 #in meters imsize = 80 # in pixels px_scalex = 65.3249 #X-dimenssion pixel scale in mas px_scaley = 65.7226 #Y-dimenssion pixel scale in mas hole_geom = 'HEXAGON' #HEXAGON for the JWST inst = 'JWST' arrname = 'DATA' ## This could be DATA or SIM. For the \"on-sky\" data use DATA rotation_angle = -0.36 ### In case the mask is not propely aligned with the position indicated in the manual oversample = 1.0 ## Number of times that you want to oversample the data. This is employed in simulations, not with real data. scale_factor = [0.996, 0.999] #Small pixel scale correction center_factor = [0,0] #Centroid position, in case alignment is needed Once the main parameters are setup, then, the main instructions are run to extract the observables. For this example, first the 3.8 \u03bcm filter is run (for both science target and calibrator) and then the observables for the 4.8 \u03bcm filter are obtained. The following lines extract the observables as described before: data_filename = 'data_WR137_bpfix_480.txt' source = 'WR137' sim_SAM.simSAM_PSF (data_filename, mask_filename, wave, bandwidth, hole_size, px_scalex, px_scaley, imsize, hole_geom, source, inst, \\ arrname, rotation_angle, oversample, scale_factor, center_factor, affine=False) data_filename = 'data_cal_bpfix_480.txt' source = 'Cal' sim_SAM.simSAM_PSF (data_filename, mask_filename, wave, bandwidth, hole_size, px_scalex, px_scaley, imsize, hole_geom, source, inst, \\ arrname, rotation_angle, oversample, scale_factor, center_factor, affine=False) ## Here, we change the filter and bandpass: wave = 3.82650365041922e-06 bandwidth = 0.205e-06 ## data_filename = 'data_WR137_bpfix_380.txt' source = 'WR137' sim_SAM.simSAM_PSF (data_filename, mask_filename, wave, bandwidth, hole_size, px_scalex, px_scaley, imsize, hole_geom, source, inst, \\ arrname, rotation_angle, oversample, scale_factor, center_factor, affine=False) data_filename = 'data_cal_bpfix_380.txt' source = 'Cal' sim_SAM.simSAM_PSF (data_filename, mask_filename, wave, bandwidth, hole_size, px_scalex, px_scaley, imsize, hole_geom, source, inst, \\ arrname, rotation_angle, oversample, scale_factor, center_factor, affine=False) The Github repository also contains the .txt files with the list of targets necessary for the reduction. As it was mentioned before, the format of the data are cubes with different frames that include the interferograms. Those files can be explored using the DS9 software . After running the test.py script, the code will generate a series of .fits files for quality check purposes, together with the final OIFITS files with the extracted observables. The list of files created by the extraction of the observables are the following ones: 1. CENTERED_*.fits files > These are the centered input files. SAMPip does a fine adjustment of the interferogram centroid in the middle of the pixel grid (this option is not yet available but the files are produced). 2. MODEL_interferogram_*.fits > These are the models of the interferograms produced by the code. They should be quite similar to the input data files. We can use the MODEL files to inspect (quickly) visually how good is our fringe modeling (i.e., visibility extraction) 3. MODEL_interferogram_windowed_*.fits > These are the models of the interferograms produced by the code but they are windowed. Only the valid pixels for the model extraction have values different from zero. 4. MODEL_residuals+*.fits > These .fits files show the residuals between our fringe model and the input data. 5. MTF.fits > This .fits file includes the Mutual Trsfer Funcion of the interferogram 6. PSF.fits > This .fits file includes the model of the interferogram produced solely by the geometry of the non-redundant mask 7. cube_bl.fits > This .fits file contains the cube of the different interference pattern produce by each pair of pin-holes in the non-redundant mask. 8. hexa.fits > This file contains the diffraction pattern of the pin-hole geometry 9. hexagon.fits > This file contains the geometry of the pin-hole mask 10. WINDDAT_*.fits > These files are the windowed input data sets used for the model extraction 11. SIM_DATA_uncalib.*.fits > These are the OFITIS files with the reduced data. They are in a standard OIFITS format and includes all the interferometric observables extracted with SAMPip. The code also produces a series of .png files to show the quality of the observables' extraction. Figs. 4 and 5 show the distribution values (from the frames in a data cube) of the squared visibilities and closure phases for the 3.8 \u03bcm filter for WR 137. The mean value, \u03c3 and the standard deviation, \u03c3 per baseline and triplet. Figure 4. Distribution of squared visibility values per baseline (21) for the NIRISS-JWST observations of WR 137 (filter F380M). Figure 5. Distribution of closure phases values per triplet (35) for the NIRISS-JWST observations of WR 137 (filter F380M). SAMPip also produces a plot with the uncalibrated squared visibilities and closure phases versus spatial frequency. Fig. 6 displays an example of those observables for WR 137. Notice how the precision in the data are quite small (sometimes even below the symbol size). This is because of the superb quality of the JWST data. Figure 6. Squared visibilities and Closure phases per spatial frequency of WR 137 (filter F380M) extracted with SAMPip. In order to check if the extraction of the observables is correct the user should check the intermediate .fits files produced by SAMPip. A quick way of deduce the quality of the observables' extraction is to look at the residuals plots to look for systematic trends. Also, one could check the MODEL_interferogram_*.fits file. If it is similar to the structure of the interferogram in the data (e.g., size, asymmetries, position, etc), it also indicates that the extraction is correct. Fig. 7 displays the data and model fits side by side. It can be easily checked that both of them exhibits the same structure. Figure 7. The left panel shows the interferogram of one of the frames in the JWST-NIRISS/SAM data cube. The right panel shows the model generated with SAMPip from which the observables are extracted. Notice thow the structure is quite similar in both panels. The scale si the same in both of them. There are a few pixels in the MODEL panel that differ from the DATA, this is because those are bad pixels filtered out by SAMPip during modeling. Once the RAW OIFITS data are obtained, it is necessary to calibrate them. For this, we use the script called calib_sam_obo.py . It contains a few instructions (see below) that calls the main calibration routine. The calibration process divide the raw science squared visibilities by the calibrator ones and subtract the calibrator's closure phases from the data ones. Fig. 8 displays the calibrated observables. #### Calibration routine script #### input_sci_path = '' # Path of the science OIFITS files input_cal_path = '' # Path of the calibrator OIFITS files calibrator_filename = 'cal_data_bpfix_380.txt' # .txt file with the Calibrator data files science_filename = 'sci_data_bpfix_380.txt' # .txt file with the Science data files cal_name = 'Cal_new' # Prefix of the calibrated OIFITS data [sc_files] = readcol.readcol(input_sci_path+science_filename, twod=False) [cal_files] = readcol.readcol(input_cal_path+calibrator_filename, twod=False) for i in range(len(sc_files)): sc_dat = sc_files[i] cal_dat = cal_files[0] delta = 1000.0 ### Hours calibrate_SAM_obo.calibrate_SAM(input_sci_path, input_cal_path, sc_dat, cal_dat, cal_name, delta) print('done') Figure 8. Calibrated squared visibilities and closure phases versus spatial frequency. Notice that the visibility values decrease with spatial frequency, showing that the target is resolved. The closure phases varies from -20 to 20 degrees, which indicates that the target is not centro-symmetric. The calibrated data can be used, thus, for imaging. Here, we just included the resulting the reconstructed images of WR 137. For comparison Fig. 9 shows the reconstructions obtained from different software and extractions obtained from different pipelines. SAMPip extractions performs similar to other pipelines that use different methods. The full scientific exploitation of the data and characterization of NIRISS-SAM are described in: Lau et al. (2023) , Kammerer et al. (2023) , and Sivaramakrishnan et al. (2023) Figure 9. Images of the morphology of WR 137. The different panels show different extraction of the observables with three pipelines, including SAMPip; and three different reconstructions. The quality of the images is comparable with the different extractions. The yellow ellipse at the bottom of the upper-rightmost panel correspond to the maxium resolution element of the interferometer. Sometimes different data sets are reduced. They can be combined with the oi_combine.fits python routine. It is a very simple script which call oi_merge.fits and js_oifits.fits . Those are two routines which reads the different data sets and write a combined OIFITS file with them. oi_merge.fits and js_oifits.fits are also included in the root directory of CASSINI/SAMPip . These two routines are also used by the during the data reduction. To combine the extracted OIFITS data, it is neccesary to create a .txt file with the oifits filenames to be combined. An example of the oi_combine.fits content is the following one: from readcol import * cd = True if cd == True: import oi_merge as oi_merge else: import oi_merge2 as oi_merges# import importlib importlib.reload(oi_merge) data = 'combine_data_tot.txt' ## Text file with the OIFITS files to be combined [files] = readcol(data, twod=False) merged = oi_merge.oi_merge(files) merged.write('COMB_JWST_SAM_tot.fits') ## Output: COMBINED OIFITS file","title":"2. Example of the use of SAMPip with the JWST  (User Manual)"},{"location":"about/","text":"This repository includes software to recover infrared interferometric images based on Compressed Sensing (CASSINI-LASSO) Link to the GitHub repository with the code, click HERE 1. Brief Introduction to Compressed Sensing Compressed Sensing (CS) allows us to recover a signal with less samples that the ones established from the Nyquist/Shannon theorem. For the technique to work, the signal must be sparse and compressible on a given basis. It means that the signal can be represented by a linear combination of functions with a small number of non-zero coefficients. In CS, a set of measurements, y , of a given signal, x , can be encoded by a multiplication of the matrices \u03a6 , \u03a8 , and the sparse vector \u03b1 . \u03a8 is the transformation basis where the full signal, x , is sparse, and only a few coefficients in the vector \u03b1 are non-zero. \u03a6 is, thus, the system of measurements under which the data are taken. For a visual representation of the matrices involved in CS see Fig. 1. It is important to remark that the number of measurements in y is considerably smaller than the number of features/columns in in \u03a8 , therefore, the inverse problem to find \u03b1 is \"ill-posed\". CS establishes that if the product \u0398 = \u03a6\u03a8 satisfies the Restricted Isometry Property (RIP), we will be able to recover the signal from the sub-sampled measurements. Therefore, compressed Sensing offers us a framework to solve the \"ill-posed\" inverse problem by a regularized optimization, using as prior the sparsity of \u03b1 and/or the degree of compressibility of the signal. This repository includes code to recover infrared interferometric images using CS from simulated Aperture Masking data. The Aperture Masking data is simulated as expected to be recorded by the near-infrared imager NIRISS on-board of the James Webb Space Telescope (JWST). Figure 1. Schematic Representation of the Compressed Sensing algorithm 2. James-Webb Space Telescope Simulations NIRISS (Near Infrared Imager and Slitless Spectrograph) is an infrared (band-pass = 0.8 - 5\u03bcm) high-resolution camera which allows us to observe an object using Fizeau interferometry in the form of Sparse Aperture Masking (SAM). We obtained the simualted data from our collaboration with the NIRISS team at the Space Telescope Science Institute (STScI). In order to reduce the interferograms of the simulatiosn, we fitted the fringes directly in the image plane using a model of the mask geometry and filter bandwidth, using our software CASSINI-SAMPip . The SAM data included as example to run the code consisted in the simulation of an inclined and asymmetric proto-planetary disk observed at three different filters (see Table 1) with the following central wavelengths: 3.8\u03bcm, 4.3\u03bcm and 4.8\u03bcm. Given the pointing limitations of the JWST, we considered a maximum of three pointing positions at a position angle(E->N) of -10 \u25e6 , 0 \u25e6 and 10 \u25e6 . To make the JWST/SAM simulations as realistic as possible, we included piston errors between 10 and 50 nm. These are typical expected error values of the instrumental transfer function. The simulated science data were calibrated with simulated interferograms of point-like objects with similar pistonerrors as the science data. The u-v coverage employed for image reconstruction includes 318 data points (V 2 + Fourier phases + CPs) and combines the different simulated pointing positions and wavelengths (see Fig. 2). Figure 2. right: u-v coverage of our JWST-SAM simulations. left: Simulated inteferogram of a proto-planetary disk as observed with the SAM mode of the JWST. Table 1. Simulated NIRISS filters. 3. Image reconstruction based on Compressed Sening The code for the reconstruction is included in the sub-directory LASSO of the main CASSINI repository. To solve the image optimization problem, the python scikit-learn library was used. More explicitly, the Least Absolute Shrinkage and Selection Operator (LASSO) algorithm was selected. This LASSO implementation uses a regularized minimization of the following form: where N is the total number of elements in the sampled signal, y, and \u03bb is the value of the hyperparameter that weights the regularizer. It is important to remark that the constraint region of the l1-norm has the form of an hypercube with several corners, which ensure sparsity of \u03b1 for a convex optimization problem. This is not the case by using, for example a Ridge regression with \u2016\u03b1\u2016 2 2 , where the constraint region is a rotational invariant n-sphere. This can also be interpreted as LASSO being a linear regression model with a Laplace prior distribution, with a sharp peak at its mean. In contrast, Gaussian prior distribution of coefficients in a Ridge regression has a more soften peak around its mean. Figure 3. The diagram shows a visual representation of the CS LASSO implementation of our work. A Dictionary of models ( \u03b8 = \u03a6\u03a8 ) is created with a group of images ( \u03a8 ), which are transformed into the measured observables atthe simulated u-v plane ( \u03a6 ). Then, the Dictionary is compared with the data ( y ) and a set of non-zero coefficients ( \u03b1 ) are selected. This process is repeated over a given number of iterations until the best-fit reconstructed image ( x ) is obtained. Before performing the minimization, a precomputed Dictionary (\u0398) with 10 4 different disk-like structures was created. The random images of the disks were created using a pixel grid of 71\u00d771 pixels with a pixelscale of 10 milliarcseconds (mas). To transform those images into the system of measurements of our data, their Fourier transform were performed using a proprietary implementation of the regularly spaced Fast Fourier Transform (FFT) and, the observables (squared visibilities, Fourier phases and closure phases) were obtained for the sampled u-v frequencies. Together with the code in this repository, we included a sample of the dictionary build from our database of models. The code create_dict.py is an example of how can we read a set of images and transform them into the Fourier Space sampled with our data. This script imports the script called oitools.py . oitools.py contains a small library of routines to perform transformations and data extraction on both, the visibility domain and the image one. One of the most interesting and, indeed, the one that is used to extract the interferometric observables from the images is oitools.compute_vis_matrix . This piece of code uses a dedicated Direct Fourier Transform to get specific amplitudes and phases from the images at the specific spatial frequencies sampled with an interferometric array. The code is the following one: def compute_vis_matrix(im, u, v, scale): import numpy as np import math import pdb sz = np.shape(im) if sz[0] % 2 == 0: x, y = np.mgrid[-np.floor(sz[1] / 2 - 1):np.floor(sz[1] / 2):sz[1] * 1j, np.floor(sz[0] / 2 - 1):-np.floor(sz[0] / 2):sz[0] * 1j] else: x, y = np.mgrid[-np.floor(sz[1] / 2):np.floor(sz[1] / 2):sz[1] * 1j, np.floor(sz[0] / 2):-np.floor(sz[0] / 2):sz[0] * 1j] x = x * scale y = y * scale xx = x.reshape(-1) yy = y.reshape(-1) im = im.reshape(-1) arg = -2.0 * math.pi * (u * yy + v * xx) reales = im.dot(np.cos(arg)) imaginarios = im.dot(np.sin(arg)) visib = np.linalg.norm([reales, imaginarios]) phase_temp = np.arctan2(imaginarios, reales) phase = np.rad2deg((phase_temp + np.pi) % (2 * np.pi) - np.pi) return visib, phase The input parameters in oitools.compute_vis_matrix are: (i) the image from which the observables are computed; (ii) the u and v coordinates of the interferometer and; (iii) the pixel scale used in the image (this input is in units of radians). create_dict.py also centered and scaled each set of observables to have a mean equals to zero and standard deviation equals to the unity. Finally, the different observables were merged into a single column vector (or atom). The different atoms were stacked to create the different columns of the final Dictionary and stored into a Python binary file. Once the Dictionary was integrated, the assembled dictionary of protoplanetary disks is included in the repository enconded into a numpy binary file. (filename = Dict3.npy) . Once the Dictionary was integrated, LASSO was used to solve for the non-zero coefficients of \u03b1 that fit the observables and reconstruct the image. LASSO worked over 10 3 iterations with a pre-defined value of the hyperparameter \u03bb. Figure 4 displays a schematic representation of the described algorithm. This is the main part of the code for recovering the images from the interferometric data. The code to run LASSO is called `CS_JWST_v1.py`. This routines uses the dictionary (in this case Dict3.npy) together with the combined data set to recover an image. The user has to modify the following entry parameters in the script: oi_data = 'COMB_JWST_SAM_tot.fits' #This is the filename of the input (COMBINED) data set dict_filename = 'Dict3.npy' #Dictionary filename input_path = ['dataset/'] # Input path for the galery of model images scale = 10.0 # Pixel scale of the generated images in milliarcseconds hyperparameter = 0.5 # Hyperparameter lambda for the LASSO minimization maxit = 10000 # Maximum number of iterations for the reconstruction output_filename = 'recovered_im_lasso.fits' #Output filename IMPORTANT: In case you want to run LASSO on your local machine, you need the DATABASE of models in the dataset/ directory. The repository includes a copy of this DATABASE but it is partially truncated because of space in the GitHub repository. Before running the code, please download the complete DATABASE from this Dropbox link and put it inside the LASSO directory before running the code. The script CS_JWST_v1.py returns the recovered image in .fits format. Figure 4 shows the best-fit image obtained with our CS LASSO algorithm together with the original model image. For the example showed in this simulation, the general structure of the disk is reproduced. The reconstructed morphology shows the correct inclination and position angle. It also shows the brighter emission of the ring along the semi-major axis. The inner cavity is also clearly observed. However, the size of the semi-minor axis is larger than the one of the model image. This can be appreciated in the map of the residuals formed by subtracting the image model from the reconstructed one. Figure 5 shows that the observables are well reproduced by the reconstructed image. Figure 4. Left: Model image from which the simulated data were obtained. Middle: Reconstructed CS LASSO image. Right: Map of residuals. The left and right panels are normalized to the unity and the color map scale is the same for an easy comparison between the two of them. Figure 5. Comparison between the data (black dots) and the recovered Squared visibilities and Closure Phases from the reconstructed CS LASSO images. The left panel displays the squared visibilities versus spatial frequency while the right panel shows the closure phases versus spatial frequencies. To verify the quality of the reconstructions with the original set of observables, the repository includes the script called plot_obs.py . This piece of code produces a PNG file called \"observables.png\". This plot is similar to Figure 5 and it shows a comparison between the extracted obsevables from the reconstructed image and the original data. The input parameters of plot_obs.py are: oi_data = 'COMB_JWST_SAM_tot.fits' #Data filename im_rec = np.squeeze(pyfits.getdata('reconvered_im_lasso_0.001.fits')) #Recovered image filename wave_range = [3.0e-6, 5.0e-6] #Wavelength range of the observations scale = 10.0 ## Pixel scale in milliarcseconds To evaluate the quality of our reconstructions, we also generated images using two other codes available in the community: BSMEM and SQUEEZE. The first one uses maximum entropy for the regularization and a gradient descent method with a trust region for the minimization. The second one could use different types of regularizations, including sparsity (in the form of the l0-norm). For the minimization a Markov-Chain Monte-Carlo (MCMC) algorithm is employed. Similar pixel scale and grid parameters between CS and the reconstructions using BSMEM and SQUEEZE were used. Fig. 6 shows the reconstructions obtained with each one of the different software. Notice that the three algorithms managed to recover the general (position angle and size) structure of the target. However, different artifacts are observed. For example, the BSMEM image shows the two brightest spots of the disk. However, it does not recover the central cavity. This can be easily explained because the Maximum Entropy enforces a smooth brightness transition between the different pixels in the image. The SQUEEZE reconstruction using the l0-norm shows a map with a \u201dgranular\u201d structure, which does not provide well defined loci for the maximum. This image does not show a clear cavity. We remark that the SQUEEZE image can be improved by using additional regularizers. Nevertheless, this is obtained at the cost of being slower. Also, the selection of the hyperaparameters becomes more complicated for more regularizers involved in the reconstruction. Both the SQUEEZE and BSMEM images show additional artifacts around the central source. This is not the case of the CS LASSO image, which shows a uniform background. To estimate the signal-to-noise ratio (SNR) between the peak of the emission and the noise floor of the images, we computed the mean value of the background using all the pixels outside a circular mask with a radius of 15 pixels centered at the middle of the image. The SNR values are: 3.7 x 10 4 , 1.0 x 10 2 and 0.8 x 10 2 for the CS LASSO, SQUEEZE and BSMEM images, respectively. These values suggest that the CS LASSO reconstruction achieved a contrast two orders of magnitude larger than the other reconstructions. This is particularly encouraging for the case of high-contrast observations as the ones expected with the JWST. Figure 7. Left: Reconstructed CS LASSO image. Middle: Reconstructed SQUEEZE image. Right: Reconstructed BSMEM image. SUMMARY To extract observables from SAM data - Run CASSINI-SAMPip Modify the script test.py or create a new one according with the data to be reduced. A description of the parameters used for the setup is included in this webpage. To combine SAM data - Run the oi_combine.py script inside the SAMpip directory. Modify the script according with the data to be combined. An oifits file is produced. A description of the parameters used for the setup is included in this webpage. To create a dictionary of structures for imaging - From a given galery of images with the desired structures, use the script create_dict.py . This code converts the images into atoms of the dictionary in the Fourier Space. For this, it uses the transformation routines included in oitools.py . The user can run create_dict.py at the root directory of the repository. This step should create a .npy binary file with the dictionary of structures to be used for the reconstruction. To recover an image - With the dictionary, run CS_JWST_v1.py . An example of the input parameters is included in this webpage. This code will produce the reconstructed image. To evaluate the quality of the reconstruction, the user can con plot_obs.py , in order to produce a plot of the synthetic observables extracted from the best-reconstructed image and the dataset. Additional tools - We recommed the user to explore oitools.py . This script contains a series of transformations and data handling routines that could be useful for evaluating the quality of the reconstructions and/or to vizualize the data.","title":"CS for SAM"},{"location":"about/#this-repository-includes-software-to-recover-infrared-interferometric-images-based-on-compressed-sensing-cassini-lasso","text":"","title":"This repository includes software to recover infrared interferometric images based on Compressed Sensing (CASSINI-LASSO)"},{"location":"about/#link-to-the-github-repository-with-the-code-click-here","text":"","title":"Link to the GitHub repository with the code, click HERE"},{"location":"about/#1-brief-introduction-to-compressed-sensing","text":"Compressed Sensing (CS) allows us to recover a signal with less samples that the ones established from the Nyquist/Shannon theorem. For the technique to work, the signal must be sparse and compressible on a given basis. It means that the signal can be represented by a linear combination of functions with a small number of non-zero coefficients. In CS, a set of measurements, y , of a given signal, x , can be encoded by a multiplication of the matrices \u03a6 , \u03a8 , and the sparse vector \u03b1 . \u03a8 is the transformation basis where the full signal, x , is sparse, and only a few coefficients in the vector \u03b1 are non-zero. \u03a6 is, thus, the system of measurements under which the data are taken. For a visual representation of the matrices involved in CS see Fig. 1. It is important to remark that the number of measurements in y is considerably smaller than the number of features/columns in in \u03a8 , therefore, the inverse problem to find \u03b1 is \"ill-posed\". CS establishes that if the product \u0398 = \u03a6\u03a8 satisfies the Restricted Isometry Property (RIP), we will be able to recover the signal from the sub-sampled measurements. Therefore, compressed Sensing offers us a framework to solve the \"ill-posed\" inverse problem by a regularized optimization, using as prior the sparsity of \u03b1 and/or the degree of compressibility of the signal. This repository includes code to recover infrared interferometric images using CS from simulated Aperture Masking data. The Aperture Masking data is simulated as expected to be recorded by the near-infrared imager NIRISS on-board of the James Webb Space Telescope (JWST). Figure 1. Schematic Representation of the Compressed Sensing algorithm","title":"1. Brief Introduction to Compressed Sensing"},{"location":"about/#2-james-webb-space-telescope-simulations","text":"NIRISS (Near Infrared Imager and Slitless Spectrograph) is an infrared (band-pass = 0.8 - 5\u03bcm) high-resolution camera which allows us to observe an object using Fizeau interferometry in the form of Sparse Aperture Masking (SAM). We obtained the simualted data from our collaboration with the NIRISS team at the Space Telescope Science Institute (STScI). In order to reduce the interferograms of the simulatiosn, we fitted the fringes directly in the image plane using a model of the mask geometry and filter bandwidth, using our software CASSINI-SAMPip . The SAM data included as example to run the code consisted in the simulation of an inclined and asymmetric proto-planetary disk observed at three different filters (see Table 1) with the following central wavelengths: 3.8\u03bcm, 4.3\u03bcm and 4.8\u03bcm. Given the pointing limitations of the JWST, we considered a maximum of three pointing positions at a position angle(E->N) of -10 \u25e6 , 0 \u25e6 and 10 \u25e6 . To make the JWST/SAM simulations as realistic as possible, we included piston errors between 10 and 50 nm. These are typical expected error values of the instrumental transfer function. The simulated science data were calibrated with simulated interferograms of point-like objects with similar pistonerrors as the science data. The u-v coverage employed for image reconstruction includes 318 data points (V 2 + Fourier phases + CPs) and combines the different simulated pointing positions and wavelengths (see Fig. 2). Figure 2. right: u-v coverage of our JWST-SAM simulations. left: Simulated inteferogram of a proto-planetary disk as observed with the SAM mode of the JWST. Table 1. Simulated NIRISS filters.","title":"2. James-Webb Space Telescope Simulations"},{"location":"about/#3-image-reconstruction-based-on-compressed-sening","text":"The code for the reconstruction is included in the sub-directory LASSO of the main CASSINI repository. To solve the image optimization problem, the python scikit-learn library was used. More explicitly, the Least Absolute Shrinkage and Selection Operator (LASSO) algorithm was selected. This LASSO implementation uses a regularized minimization of the following form: where N is the total number of elements in the sampled signal, y, and \u03bb is the value of the hyperparameter that weights the regularizer. It is important to remark that the constraint region of the l1-norm has the form of an hypercube with several corners, which ensure sparsity of \u03b1 for a convex optimization problem. This is not the case by using, for example a Ridge regression with \u2016\u03b1\u2016 2 2 , where the constraint region is a rotational invariant n-sphere. This can also be interpreted as LASSO being a linear regression model with a Laplace prior distribution, with a sharp peak at its mean. In contrast, Gaussian prior distribution of coefficients in a Ridge regression has a more soften peak around its mean. Figure 3. The diagram shows a visual representation of the CS LASSO implementation of our work. A Dictionary of models ( \u03b8 = \u03a6\u03a8 ) is created with a group of images ( \u03a8 ), which are transformed into the measured observables atthe simulated u-v plane ( \u03a6 ). Then, the Dictionary is compared with the data ( y ) and a set of non-zero coefficients ( \u03b1 ) are selected. This process is repeated over a given number of iterations until the best-fit reconstructed image ( x ) is obtained. Before performing the minimization, a precomputed Dictionary (\u0398) with 10 4 different disk-like structures was created. The random images of the disks were created using a pixel grid of 71\u00d771 pixels with a pixelscale of 10 milliarcseconds (mas). To transform those images into the system of measurements of our data, their Fourier transform were performed using a proprietary implementation of the regularly spaced Fast Fourier Transform (FFT) and, the observables (squared visibilities, Fourier phases and closure phases) were obtained for the sampled u-v frequencies. Together with the code in this repository, we included a sample of the dictionary build from our database of models. The code create_dict.py is an example of how can we read a set of images and transform them into the Fourier Space sampled with our data. This script imports the script called oitools.py . oitools.py contains a small library of routines to perform transformations and data extraction on both, the visibility domain and the image one. One of the most interesting and, indeed, the one that is used to extract the interferometric observables from the images is oitools.compute_vis_matrix . This piece of code uses a dedicated Direct Fourier Transform to get specific amplitudes and phases from the images at the specific spatial frequencies sampled with an interferometric array. The code is the following one: def compute_vis_matrix(im, u, v, scale): import numpy as np import math import pdb sz = np.shape(im) if sz[0] % 2 == 0: x, y = np.mgrid[-np.floor(sz[1] / 2 - 1):np.floor(sz[1] / 2):sz[1] * 1j, np.floor(sz[0] / 2 - 1):-np.floor(sz[0] / 2):sz[0] * 1j] else: x, y = np.mgrid[-np.floor(sz[1] / 2):np.floor(sz[1] / 2):sz[1] * 1j, np.floor(sz[0] / 2):-np.floor(sz[0] / 2):sz[0] * 1j] x = x * scale y = y * scale xx = x.reshape(-1) yy = y.reshape(-1) im = im.reshape(-1) arg = -2.0 * math.pi * (u * yy + v * xx) reales = im.dot(np.cos(arg)) imaginarios = im.dot(np.sin(arg)) visib = np.linalg.norm([reales, imaginarios]) phase_temp = np.arctan2(imaginarios, reales) phase = np.rad2deg((phase_temp + np.pi) % (2 * np.pi) - np.pi) return visib, phase The input parameters in oitools.compute_vis_matrix are: (i) the image from which the observables are computed; (ii) the u and v coordinates of the interferometer and; (iii) the pixel scale used in the image (this input is in units of radians). create_dict.py also centered and scaled each set of observables to have a mean equals to zero and standard deviation equals to the unity. Finally, the different observables were merged into a single column vector (or atom). The different atoms were stacked to create the different columns of the final Dictionary and stored into a Python binary file. Once the Dictionary was integrated, the assembled dictionary of protoplanetary disks is included in the repository enconded into a numpy binary file. (filename = Dict3.npy) . Once the Dictionary was integrated, LASSO was used to solve for the non-zero coefficients of \u03b1 that fit the observables and reconstruct the image. LASSO worked over 10 3 iterations with a pre-defined value of the hyperparameter \u03bb. Figure 4 displays a schematic representation of the described algorithm. This is the main part of the code for recovering the images from the interferometric data. The code to run LASSO is called `CS_JWST_v1.py`. This routines uses the dictionary (in this case Dict3.npy) together with the combined data set to recover an image. The user has to modify the following entry parameters in the script: oi_data = 'COMB_JWST_SAM_tot.fits' #This is the filename of the input (COMBINED) data set dict_filename = 'Dict3.npy' #Dictionary filename input_path = ['dataset/'] # Input path for the galery of model images scale = 10.0 # Pixel scale of the generated images in milliarcseconds hyperparameter = 0.5 # Hyperparameter lambda for the LASSO minimization maxit = 10000 # Maximum number of iterations for the reconstruction output_filename = 'recovered_im_lasso.fits' #Output filename IMPORTANT: In case you want to run LASSO on your local machine, you need the DATABASE of models in the dataset/ directory. The repository includes a copy of this DATABASE but it is partially truncated because of space in the GitHub repository. Before running the code, please download the complete DATABASE from this Dropbox link and put it inside the LASSO directory before running the code. The script CS_JWST_v1.py returns the recovered image in .fits format. Figure 4 shows the best-fit image obtained with our CS LASSO algorithm together with the original model image. For the example showed in this simulation, the general structure of the disk is reproduced. The reconstructed morphology shows the correct inclination and position angle. It also shows the brighter emission of the ring along the semi-major axis. The inner cavity is also clearly observed. However, the size of the semi-minor axis is larger than the one of the model image. This can be appreciated in the map of the residuals formed by subtracting the image model from the reconstructed one. Figure 5 shows that the observables are well reproduced by the reconstructed image. Figure 4. Left: Model image from which the simulated data were obtained. Middle: Reconstructed CS LASSO image. Right: Map of residuals. The left and right panels are normalized to the unity and the color map scale is the same for an easy comparison between the two of them. Figure 5. Comparison between the data (black dots) and the recovered Squared visibilities and Closure Phases from the reconstructed CS LASSO images. The left panel displays the squared visibilities versus spatial frequency while the right panel shows the closure phases versus spatial frequencies. To verify the quality of the reconstructions with the original set of observables, the repository includes the script called plot_obs.py . This piece of code produces a PNG file called \"observables.png\". This plot is similar to Figure 5 and it shows a comparison between the extracted obsevables from the reconstructed image and the original data. The input parameters of plot_obs.py are: oi_data = 'COMB_JWST_SAM_tot.fits' #Data filename im_rec = np.squeeze(pyfits.getdata('reconvered_im_lasso_0.001.fits')) #Recovered image filename wave_range = [3.0e-6, 5.0e-6] #Wavelength range of the observations scale = 10.0 ## Pixel scale in milliarcseconds To evaluate the quality of our reconstructions, we also generated images using two other codes available in the community: BSMEM and SQUEEZE. The first one uses maximum entropy for the regularization and a gradient descent method with a trust region for the minimization. The second one could use different types of regularizations, including sparsity (in the form of the l0-norm). For the minimization a Markov-Chain Monte-Carlo (MCMC) algorithm is employed. Similar pixel scale and grid parameters between CS and the reconstructions using BSMEM and SQUEEZE were used. Fig. 6 shows the reconstructions obtained with each one of the different software. Notice that the three algorithms managed to recover the general (position angle and size) structure of the target. However, different artifacts are observed. For example, the BSMEM image shows the two brightest spots of the disk. However, it does not recover the central cavity. This can be easily explained because the Maximum Entropy enforces a smooth brightness transition between the different pixels in the image. The SQUEEZE reconstruction using the l0-norm shows a map with a \u201dgranular\u201d structure, which does not provide well defined loci for the maximum. This image does not show a clear cavity. We remark that the SQUEEZE image can be improved by using additional regularizers. Nevertheless, this is obtained at the cost of being slower. Also, the selection of the hyperaparameters becomes more complicated for more regularizers involved in the reconstruction. Both the SQUEEZE and BSMEM images show additional artifacts around the central source. This is not the case of the CS LASSO image, which shows a uniform background. To estimate the signal-to-noise ratio (SNR) between the peak of the emission and the noise floor of the images, we computed the mean value of the background using all the pixels outside a circular mask with a radius of 15 pixels centered at the middle of the image. The SNR values are: 3.7 x 10 4 , 1.0 x 10 2 and 0.8 x 10 2 for the CS LASSO, SQUEEZE and BSMEM images, respectively. These values suggest that the CS LASSO reconstruction achieved a contrast two orders of magnitude larger than the other reconstructions. This is particularly encouraging for the case of high-contrast observations as the ones expected with the JWST. Figure 7. Left: Reconstructed CS LASSO image. Middle: Reconstructed SQUEEZE image. Right: Reconstructed BSMEM image.","title":"3. Image reconstruction based on Compressed Sening"},{"location":"about/#summary","text":"To extract observables from SAM data - Run CASSINI-SAMPip Modify the script test.py or create a new one according with the data to be reduced. A description of the parameters used for the setup is included in this webpage. To combine SAM data - Run the oi_combine.py script inside the SAMpip directory. Modify the script according with the data to be combined. An oifits file is produced. A description of the parameters used for the setup is included in this webpage. To create a dictionary of structures for imaging - From a given galery of images with the desired structures, use the script create_dict.py . This code converts the images into atoms of the dictionary in the Fourier Space. For this, it uses the transformation routines included in oitools.py . The user can run create_dict.py at the root directory of the repository. This step should create a .npy binary file with the dictionary of structures to be used for the reconstruction. To recover an image - With the dictionary, run CS_JWST_v1.py . An example of the input parameters is included in this webpage. This code will produce the reconstructed image. To evaluate the quality of the reconstruction, the user can con plot_obs.py , in order to produce a plot of the synthetic observables extracted from the best-reconstructed image and the dataset. Additional tools - We recommed the user to explore oitools.py . This script contains a series of transformations and data handling routines that could be useful for evaluating the quality of the reconstructions and/or to vizualize the data.","title":"SUMMARY"},{"location":"cs_pca/","text":"This repository includes software to perform Principal Component Analysis for infrared interferometric imaging based on Compressed Sensing (CASSINI-PCA V1.0) Link to the GitHub repository with the code, click HERE 1. Principal Component Analysis Principal Component Analysis, or PCA, is a dimensionality-reduction method, which transform a large set of variables into a smaller one that still contains most of the information in the large set. In this repository, we include a series of scripts to perform PCA on recovered infrared interferometric images. These images could be obtained with CASSINI-LASSO or with any other image reconstruction software (e.g., BSMEM , SQUEEZE or MiRA ). The purpose of applying PCA to interferometric images is to evaluate the quality of the reconstructions in terms of the features observed in the image. Very recently Medeiros et al. (2020) has demonstrated that PCA is a potential technique to characterize variability in reconstructed images from sparse interferometric data. Those authors demonstrated that the Fourier transforms of eigenimages derived from PCA applied to an ensemble of images in the spatial-domain are identical to the eigenvectors of PCA applied to the ensemble of the Fourier transforms of the images. This means that the visibilities of interferometric data can be represented as a linear combination of the Fourier transform of the principal components of the reconstructed images. 2. Visibilities from PCA As example on the usability of the code, we include reconstructed images from data obtained with GRAVITY-VLTI data using SQUEEZE. The cube of images is included together with the code in the CASSINI-PCA directory. The data cube consists in 307 reconstruced images (513 x 513 pixels) of a compact young stellar object. Fig. 1 shows the mean image out of the 307 frames in the cube. It can be appreciated that the image shows a central elongated object. Also, two secondary lobes, due to the sparsity of the u-v plane, are observed in the image. Figure 1. Mean reconstructed image of the source used for PCA analysis. The code uses the following input parameters: ####### The following parameters are necessary to run CASSINI/PCA ################## cube_filename = 'cube_example.fits' ## The data cube with the reconstructed images oifits_filename = 'oifits_example.fits' ## The oifits file with the interferometric data to be modeled ### For fitting 1 ##### n_comp = 50 ###Number of components to be extracted from the image, it has to be less or equal than the number of frames in the cube scale = 0.1 ### Pixel scale used in the reconstruction of the images (mas) n_comp_compiled = 10 ## Number of components to be added and stored into a .fits file for visualization purposes (it has to be <= n_comp) display = True ## Bolean variable, it is used to display a plot of the 10 principal components in the image (in n_comp has to be equal or larger than 10) #lambd_values = [0.01] lambd_values1 = np.logspace(-2, 3, 5) PCA_im.py computes number of principal components defined with n_comp . The variable n_comp_compiled defines the number of principal components to be used to recover the visibilities through a minimization based on compressed sensing. The n_comp_compiled value has to be equal or lower to the total number of principal components computed by the code. In case display=True the code wil produce a plot of the initial 10 components (see Fig. 2). NOTE: The number of extracted components should be larger or equal than 10. Otherwise, the code will complain/crash!. Figure 2. Panels with the 10 different components produced by the data in the example included with the code. The lambd_values1 parameter corresponds to the hyperparameter values used for the compressed sensing computation of the visibilities from the dictionary of structures extracted from the Principal Components of the image cube. lambd_values1 could take a float or an array of values. By default, the code will solve the compressed sensing problem using the Python package cvxpy . However, the code allows the user to use other two Python packages, sklearn.decomposition and PyLops , to solve the Compressed Sensing reconstruction of the vislibilities. In order to enable these options, the user has to declare as True the following two boolean variables: sklearn_opt = True lambd_values2 = np.logspace(-2, 3, 5) ##Lambda values for CS solver using scikit-learn pylops_opt = True lambd_values3 = np.logspace(-2, 3, 5) ##Lambda values for CS solver using pylops Notice that the lambd_values2 and lambd_values3 should be declared as float or array. Notice that these values could be similar to lambd_values1 . However, they could produce different results. This is because the different Python solvers use different methods for finding the eigenvalues (\u03b1). Nevertheless, the purpose of using different solvers is that the user could have the possibility to check how these results affect the reconstruction based on the morphological properties of the source in the image. Each compressed sensing solver produces a cube of images stored in .fits files. A .fits file is produced for each hyperparameter used. Each .fits cube contains with the principal components from the image weighted by their corresponding eigenvalues to recover the visibility values. The user can use these cube of images to evaluate the contribution of the different components to the final recovered structure of the target. For each compressed sensing solver, the code produces a plot with the reconstruction of the visibilities and phases from the number of components defined in n_comp_compiled for each one of the hyperparameter value used. Fig. 3 shows an example of this plot, similar plots are also produced if the other two solvers are enabled. Figure 3. Visibilities recovered from the principal components of the cube of reconstructed images. The visibility amplitudes (upper panel) and the phases (lower panel) are recovered using a compressed sensing solver of the Fourier Transform of the principal components at the u-v frequencies traced with the inteferometer. The compressed sensing solver uses the number of components defined in n_comp_compiled . The different colors in the plot indicates the visibility extractions using different hyperparameter (\u03bb) values. For the default cvxpy solver, the code also produces an image that traces the regularization path of the different sparse eigenvalues for which each principal component image is multiplied. Notice that for an extreme case, where the hyperparameter \u03bb over-regularizes the data, the eigenvalues beta decay to zero (see Fig. 4). Figure 4. Visibilities recovered from the principal components of the cube of reconstructed images. The visibility amplitudes (upper panel) and the phases (lower panel) are reovered using a compressed sensing solver of the Fourier Transform of the principal components at the u-v frequencies traced with the inteferometer. The compressed sensins solver uses the number of components defined in n_comp_compiled . The different colors in the plot indicates the visibility extractions using different hyperparameter (\u03bb) values. For the sklearn solver, the code also produces an image that displays the number of non-zero coefficients in the \u03b1 values for the different hyperameter (\u03bb) values used in the reconstruction (see Figure 5). Figure 5. Number of non-zero \u03b1 coefficients for each visibility reconstruction for a given hyperparameter","title":"CS PCA"},{"location":"cs_pca/#this-repository-includes-software-to-perform-principal-component-analysis-for-infrared-interferometric-imaging-based-on-compressed-sensing-cassini-pca-v10","text":"","title":"This repository includes software to perform Principal Component Analysis for infrared interferometric imaging based on Compressed Sensing (CASSINI-PCA V1.0)"},{"location":"cs_pca/#link-to-the-github-repository-with-the-code-click-here","text":"","title":"Link to the GitHub repository with the code, click HERE"},{"location":"cs_pca/#1-principal-component-analysis","text":"Principal Component Analysis, or PCA, is a dimensionality-reduction method, which transform a large set of variables into a smaller one that still contains most of the information in the large set. In this repository, we include a series of scripts to perform PCA on recovered infrared interferometric images. These images could be obtained with CASSINI-LASSO or with any other image reconstruction software (e.g., BSMEM , SQUEEZE or MiRA ). The purpose of applying PCA to interferometric images is to evaluate the quality of the reconstructions in terms of the features observed in the image. Very recently Medeiros et al. (2020) has demonstrated that PCA is a potential technique to characterize variability in reconstructed images from sparse interferometric data. Those authors demonstrated that the Fourier transforms of eigenimages derived from PCA applied to an ensemble of images in the spatial-domain are identical to the eigenvectors of PCA applied to the ensemble of the Fourier transforms of the images. This means that the visibilities of interferometric data can be represented as a linear combination of the Fourier transform of the principal components of the reconstructed images.","title":"1. Principal Component Analysis"},{"location":"cs_pca/#2-visibilities-from-pca","text":"As example on the usability of the code, we include reconstructed images from data obtained with GRAVITY-VLTI data using SQUEEZE. The cube of images is included together with the code in the CASSINI-PCA directory. The data cube consists in 307 reconstruced images (513 x 513 pixels) of a compact young stellar object. Fig. 1 shows the mean image out of the 307 frames in the cube. It can be appreciated that the image shows a central elongated object. Also, two secondary lobes, due to the sparsity of the u-v plane, are observed in the image. Figure 1. Mean reconstructed image of the source used for PCA analysis. The code uses the following input parameters: ####### The following parameters are necessary to run CASSINI/PCA ################## cube_filename = 'cube_example.fits' ## The data cube with the reconstructed images oifits_filename = 'oifits_example.fits' ## The oifits file with the interferometric data to be modeled ### For fitting 1 ##### n_comp = 50 ###Number of components to be extracted from the image, it has to be less or equal than the number of frames in the cube scale = 0.1 ### Pixel scale used in the reconstruction of the images (mas) n_comp_compiled = 10 ## Number of components to be added and stored into a .fits file for visualization purposes (it has to be <= n_comp) display = True ## Bolean variable, it is used to display a plot of the 10 principal components in the image (in n_comp has to be equal or larger than 10) #lambd_values = [0.01] lambd_values1 = np.logspace(-2, 3, 5) PCA_im.py computes number of principal components defined with n_comp . The variable n_comp_compiled defines the number of principal components to be used to recover the visibilities through a minimization based on compressed sensing. The n_comp_compiled value has to be equal or lower to the total number of principal components computed by the code. In case display=True the code wil produce a plot of the initial 10 components (see Fig. 2). NOTE: The number of extracted components should be larger or equal than 10. Otherwise, the code will complain/crash!. Figure 2. Panels with the 10 different components produced by the data in the example included with the code. The lambd_values1 parameter corresponds to the hyperparameter values used for the compressed sensing computation of the visibilities from the dictionary of structures extracted from the Principal Components of the image cube. lambd_values1 could take a float or an array of values. By default, the code will solve the compressed sensing problem using the Python package cvxpy . However, the code allows the user to use other two Python packages, sklearn.decomposition and PyLops , to solve the Compressed Sensing reconstruction of the vislibilities. In order to enable these options, the user has to declare as True the following two boolean variables: sklearn_opt = True lambd_values2 = np.logspace(-2, 3, 5) ##Lambda values for CS solver using scikit-learn pylops_opt = True lambd_values3 = np.logspace(-2, 3, 5) ##Lambda values for CS solver using pylops Notice that the lambd_values2 and lambd_values3 should be declared as float or array. Notice that these values could be similar to lambd_values1 . However, they could produce different results. This is because the different Python solvers use different methods for finding the eigenvalues (\u03b1). Nevertheless, the purpose of using different solvers is that the user could have the possibility to check how these results affect the reconstruction based on the morphological properties of the source in the image. Each compressed sensing solver produces a cube of images stored in .fits files. A .fits file is produced for each hyperparameter used. Each .fits cube contains with the principal components from the image weighted by their corresponding eigenvalues to recover the visibility values. The user can use these cube of images to evaluate the contribution of the different components to the final recovered structure of the target. For each compressed sensing solver, the code produces a plot with the reconstruction of the visibilities and phases from the number of components defined in n_comp_compiled for each one of the hyperparameter value used. Fig. 3 shows an example of this plot, similar plots are also produced if the other two solvers are enabled. Figure 3. Visibilities recovered from the principal components of the cube of reconstructed images. The visibility amplitudes (upper panel) and the phases (lower panel) are recovered using a compressed sensing solver of the Fourier Transform of the principal components at the u-v frequencies traced with the inteferometer. The compressed sensing solver uses the number of components defined in n_comp_compiled . The different colors in the plot indicates the visibility extractions using different hyperparameter (\u03bb) values. For the default cvxpy solver, the code also produces an image that traces the regularization path of the different sparse eigenvalues for which each principal component image is multiplied. Notice that for an extreme case, where the hyperparameter \u03bb over-regularizes the data, the eigenvalues beta decay to zero (see Fig. 4). Figure 4. Visibilities recovered from the principal components of the cube of reconstructed images. The visibility amplitudes (upper panel) and the phases (lower panel) are reovered using a compressed sensing solver of the Fourier Transform of the principal components at the u-v frequencies traced with the inteferometer. The compressed sensins solver uses the number of components defined in n_comp_compiled . The different colors in the plot indicates the visibility extractions using different hyperparameter (\u03bb) values. For the sklearn solver, the code also produces an image that displays the number of non-zero coefficients in the \u03b1 values for the different hyperameter (\u03bb) values used in the reconstruction (see Figure 5). Figure 5. Number of non-zero \u03b1 coefficients for each visibility reconstruction for a given hyperparameter","title":"2. Visibilities from PCA"}]}